{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Welcome to Arista CI Workshops","text":"<p> </p> <p>The Arista CI Workshops are intended for engineers looking to learn the fundamentals of automation tools and get hands-on experience deploying network-wide configurations with Arista Validated Designs (AVD). The workshops are split into two in-person sessions allowing time to grasp the basic automation concepts before moving into building Data Models to deploy AVD. The content on this site is an overview of the concepts we will cover in person with full details and examples.</p> <ul> <li>Workshop #1 - Automation Fundamentals 101</li> <li>Workshop #2 - Arista CI - AVD and CI/CD</li> </ul>","location":""},{"title":"Fundamentals","text":"<ul> <li>Git</li> <li>VS Code</li> <li>Jinja/YAML</li> <li>Ansible</li> </ul>","location":"#fundamentals"},{"title":"Arista CI","text":"<ul> <li>AVD<ul> <li>Overview</li> <li>Lab Guide</li> </ul> </li> <li>CI/CD Basics</li> </ul>","location":"#arista-ci"},{"title":"Welcome to Ansible","text":"<p></p>","location":"ansible/"},{"title":"What is Ansible","text":"<p>Ansible is a Python-based automation framework. Today, the term \"Ansible Automation Platform\" can refer to multiple applications, including:</p> <ul> <li>Ansible Core</li> <li>Ansible Galaxy</li> <li>Ansible Automation Controller (Previously known as Tower)</li> <li>Red Hat Insights</li> </ul> <p>We will be focusing on the first two items during this workshop.</p>","location":"ansible/#what-is-ansible"},{"title":"Why use Ansible","text":"<p>Aside from being \"agentless\", meaning that Ansible does not require any specialized software on the target hosts, Ansible is also straightforward to get started with. While prior coding or experience in automation is helpful, it is optional to get up and running with Ansible. Playbooks are written in YAML, a language that we'll cover in detail in the YAML section. For now, rest assured that YAML is a human-readable language, which is why it's accessible to start our Ansible journey.</p> <p>There is an extensive and very active user and development community with Ansible. The project itself is open source, with the GitHub repository available here. The popularity of Ansible has led to broad vendor support, spanning multiple technology silos. Network, Compute, Storage, Cloud, Security, and more can all be automated via Ansible.</p> <p>Finally, all that is required to get started is a Linux host with Python installed. A single Ansible Control Node (ACN) can manage hundreds, or thousands, of endpoints.</p>  Important Note Before Getting Started <p>This section will make use of the fork of the Workshops GitHub repository that was created during the Git section. If you have not made a fork of this repository and cloned it into the <code>/home/coder/project/labfiles/</code> directory of your lab environment's VS Code IDE, please do so before moving forward.</p>","location":"ansible/#why-use-ansible"},{"title":"Ansible Initial Setup","text":"<p>There are multiple methods of installing Ansible on the Ansible Control Node. The most popular way is to leverage <code>pip</code>, and is covered in detail here.</p> <p>We will be using <code>ansible-core</code>, which is a lightweight minimalist installation of Ansible without any extra modules, plugins, etc. included. With this approach, we can use Ansible Galaxy (covered later in this section) to install collections containing the modules, plugins, and roles we need. For those familiar with Python, think of Ansible Galaxy as pypi.org, and Ansible Collections as Python modules.</p>  Note <p>Ansible is already installed in the Arista Test Drive lab topology, so we won't need to perform any installation-related tasks.</p>  <p>Below is an example of installing <code>ansible-core</code> via pip on Ubuntu 20.04:</p> <pre><code>#If necessary, install Python3\nsudo apt install update &amp;&amp; upgrade\nsudo apt install python3 python3-pip --yes\n\n# Install Ansible Core\npip3 install ansible-core\n</code></pre> <p>It is that easy to get started!</p> <p>Before running any commands, let's first ensure that we're in the <code>/home/coder/project/labfiles/ci-workshops-fundamentals/ansible</code> directory.</p> <pre><code>cd ~/project/labfiles/ci-workshops-fundamentals/ansible\n</code></pre> <p>Next, we'll confirm that Ansible is installed by running the <code>ansible --version</code> in the terminal. This should yield output similar to below:</p> <pre><code>ansible [core 2.12.10]\n  config file = /home/coder/project/labfiles/ci-workshops-fundamentals/ansible/ansible.cfg\n  configured module search path = ['/home/coder/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /home/coder/.local/lib/python3.9/site-packages/ansible\n  ansible collection location = /home/coder/.ansible/collections\n  executable location = /home/coder/.local/bin/ansible\n  python version = 3.9.2 (default, Feb 28 2021, 17:03:44) [GCC 10.2.1 20210110]\n  jinja version = 3.1.2\n  libyaml = True\n</code></pre> <p>All of the information displayed above is important. It can help troubleshoot why something may not work as expected when working with Ansible.</p>","location":"ansible/#ansible-initial-setup"},{"title":"Control Node and Managed Nodes","text":"<p></p> <p>Two important initial terms are the Ansible <code>Control Node</code> and <code>Managed Node</code>.</p> <p>The Control Node is where our playbooks are executed from. This Node then connects to the Managed Nodes to interact with them to perform the desired tasks. How the Control Node interacts with the Managed Node depends on the type of operating system running on the Managed Node. For example, suppose the Managed Node is a Linux server. In that case, the Control Node will \"ship\" the Python code associated with the tasks to the Managed Node. Then, the Managed Node will locally execute that code to complete the tasks.</p>  What about the lab environment? <p>In our ATD lab environment, the <code>Control Node</code> is our JumpHost from which we're running our VS Code IDE. The <code>Managed Nodes</code> are switches that make up our lab's network topology.</p>  <p>Suppose the Managed Node is a network device. In that case, the Control Node will locally execute the Python code associated with the tasks and then interact with the network devices via SSH or API to complete the tasks.</p> <p>The Control Node must be a Linux host (Ubuntu, CentOS, Rocky, Debian, etc.) with Ansible installed. That's it! Really! This is part of what makes Ansible easy and efficient to get started with. It does not require a software suite to be installed to get started. A single Control Node can manage hundreds or thousands of Managed Nodes.</p> <p>A Managed Node does not need any specialized software installed. In other words, Ansible is <code>agentless</code>. If a Managed Node is a Linux server, it must have Python3 installed. However, no prerequisites are required for Managed Nodes that are network devices, not even Python. This is because the Control Node will locally execute the Python code necessary to complete the tasks on the network device and will then interact as needed with the network device via SSH/API.</p>","location":"ansible/#control-node-and-managed-nodes"},{"title":"Ansible Components","text":"<p>Like any other framework, Ansible is made up of a group of components that come together to make the magic happen. The components we'll focus on during this workshop are shown below.</p> <p></p> <p>In the next sections, we'll review these components individually to understand of how each piece fits together.</p>","location":"ansible/#ansible-components"},{"title":"Config File","text":"<p>The Ansible configuration file is where we set environment variables for our Ansible projects. Many variables can be set in this file, and the most common ones are documented here.</p> <p>When running an ad-hoc command, or playbook, Ansible will look for the configuration file in the locations listed below. These locations are defined in order of precedence:</p> <ol> <li> <p>ANSIBLE_CONFIG (environment variable if set)</p> </li> <li> <p>ansible.cfg (in the current directory)</p> </li> <li> <p>~/.ansible.cfg (in the home directory)</p> </li> <li> <p>/etc/ansible/ansible.cfg</p> </li> </ol> <p>This is illustrated in the image below:</p> <p></p> <p>Once Ansible finds an <code>ansible.cfg</code> file, it will only use the configuration options defined in that file. If, for example, an <code>ansible.cfg</code> file exists in the current directory and in <code>/etc/ansible/ansible.cfg</code>, then only the settings found in the <code>ansible.cfg</code> file in the current directory will be used.</p>  Cows? <p>Yes! First, install cowsay: <code>sudo apt-get update &amp;&amp; sudo apt-get install cowsay -y</code> Next, in the <code>ansible.cfg</code> file, set it to what should be the only acceptable setting, <code>nocows = False</code>. This feature is udderly ridiculous, and as much as I'd love to milk it for all the Dad jokes possible, I don't want to start any beef by delaying the workshop...Moo.</p>  <p>Below is an example of the <code>ansible.cfg</code> located in our fork of the Workshops repo:</p>  Example Ansible Configuration File (~/project/labfiles/ci-workshops-fundamentals/ansible/ansible.cfg) <pre><code>[defaults]\n\n# Disable host key checking by the underlying tools Ansible uses to connect to target hosts\nhost_key_checking = False\n\n# Location of inventory file containing target hosts\ninventory = ./inventory/inventory.yml\n\n# Only gather Ansible facts if explicity directed to in a given play\ngathering = explicit\n\n# Disable the creation of .retry files if a playbook fails\nretry_files_enabled = False\n\n# Path(s) to search for installed Ansible Galaxy Collections\ncollections_paths = ~/.ansible/collections\n\n# Enable additional Jinja2 Extensions (https://jinja.palletsprojects.com/en/3.1.x/extensions/)\njinja2_extensions =  jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n\n\n# Enable the YAML callback plugin, providing much easier to read terminal output. (https://docs.ansible.com/ansible/latest/plugins/callback.html#callback-plugins)\nstdout_callback = yaml\n\n# Permit the use of callback plugins when running ad-hoc commands\nbin_ansible_callbacks = True\n\n# List of enabled callbacks. Many callbacks shipped with Ansible are not enabled by default\ncallback_whitelist = profile_roles, profile_tasks, timer\n\n# Maximum number of forks that Ansible will use to execute tasks on target hosts\nforks = 15\n\n# Disable cowsay (Why?)\nnocows = True\n\n[paramiko_connection]\n# Automatically add the keys of target hosts to known hosts\nhost_key_auto_add = True\n\n[persistent_connection]\n# Set the amount of time, in seconds, to wait for response from remote device before timing out persistent connection.\ncommand_timeout = 60\n\n# Set the amount of time, in seconds, that a persistent connection will remain idle before it is destroyed.\nconnect_timeout = 60\n</code></pre>  <p>One of the most common settings in the ansible.cfg file is the location of the <code>inventory</code> file, which we will discuss next.</p>","location":"ansible/#config-file"},{"title":"Inventory","text":"<p>In the inventory file, we define the hosts and groups we'll target with our playbooks. The inventory file supports many formats, but the most common are <code>ini</code> and <code>yaml</code>. For our workshop, we'll be using the <code>yaml</code> format.</p> <p>An example of our inventory file can be seen below:</p>  Example Inventory File (~project/labfiles/ci-workshops-fundamentals/ansible/inventory/inventory.yml) <pre><code>WORKSHOP_FABRIC:\n  children:\n    S1:\n      children:\n        S1_SPINES:\n          hosts:\n            s1-spine1:\n            s1-spine2:\n        S1_LEAFS:\n          hosts:\n            s1-leaf1:\n            s1-leaf2:\n            s1-leaf3:\n            s1-leaf4:\n            s1-brdr1:\n            s1-brdr2:\n            s1-core1:\n            s1-core2:\n    S2:\n      children:\n        S2_SPINES:\n          hosts:\n            s2-spine1:\n            s2-spine2:\n        S2_LEAFS:\n          hosts:\n            s2-leaf1:\n            s2-leaf2:\n            s2-leaf3:\n            s2-leaf4:\n            s2-brdr1:\n            s2-brdr2:\n            s2-core1:\n            s2-core2:\n</code></pre>  <p>This inventory file defines the hosts and groupings, represented in the image below:</p> <p></p> <p>We can validate our inventory by using the <code>ansible-inventory</code> command, which is documented here.</p> <p>The below command will list the entire inventory, consisting of all hosts/groups and their respective variable values.</p> <pre><code>ansible-inventory --list --yaml\n</code></pre> <p>If we'd like to get more specific, we can filter the output down to a single host by using the command shown below:</p> <pre><code>ansible-inventory --host s1-leaf1 --yaml\n</code></pre>  Example inventory output for s1-leaf1 <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from host_vars/s1-leaf1.YML\nmlag:\n  enabled: true\n  peer_link_int_1: 1\n  peer_link_int_2: 6\n  side_a: true\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre>  <p>Notice the variables associated with <code>s1-leaf1</code>. Where did these come from? We'll be exploring that next...</p>","location":"ansible/#inventory"},{"title":"Variables","text":"<p>We can define variables in many locations with Ansible. For example, we can explicitly define a variable when running a playbook by using the <code>extra-vars</code> flag.</p>  Reminder <p>Ensure all commands are run from the <code>/home/coder/project/labfiles/ci-workshops-fundamentals/ansible</code> directory in the terminal on the ATD VS Code IDE instance.</p>  <pre><code>ansible-playbook playbooks/hello_world.yml -e 'name=Mitch'\n</code></pre> <p>The contents of the playbook we just ran can be seen below. We will look more into the anatomy of a playbook in our next section. For now, note that the <code>name</code> variable is also set in the <code>hello-world.yml</code> playbook using the <code>vars</code> parameter. When we ran our playbook with <code>extra_vars</code>, this took precedence over the variable defined inside the playbook.</p>  hello_world.yml Playbook (~/project/labfiles/ci-workshops-fundamentals/ansible/playbooks/hello_world.yml) <pre><code>---\n\n- name: A simple playbook\n  hosts: localhost\n  gather_facts: false\n  vars:\n    name: Mr.T\n\n  tasks:\n\n    - name: Say Hello\n      debug:\n        msg: \"Hello {{ name | default('you!') }}\"\n</code></pre>  <p>Other valid locations for variables, and their respective precedence, are shown in the diagram below:</p> <p></p> <p>As can be expected, there are a LOT of places where we can define variables. Each option has its use case, but the general recommendation is to use host_vars and group_vars as much as possible.</p> <p>Inside our <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory</code> directory, is a <code>host_vars</code> and <code>group_vars</code> directory. These are special directories that Ansible will use to establish a hierarchy of variables that maps directly to our inventory hosts and group structure. Each group can have a dedicated <code>yaml</code> file, as can each host. A visual representation of this can be seen below:</p> <p></p> <p>In the example above, if we were to define a variable in the <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/WORKSHOP_FABRIC.yml</code> file, then all Managed Nodes contained within that group in our inventory file would inherit that variable. The contents of our <code>WORKSHOP_FABRIC.yml</code> file can be seen below:</p>  WORKSHOP_FABRIC.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/WORKSHOP_FABRIC.yml) <pre><code># eAPI connectivity via HTTPS (as opposed to CLI via SSH)\nansible_connection: ansible.netcommon.httpapi\n\n# Specifies that we are using Arista EOS\nansible_network_os: arista.eos.eos\n\n# Use SSL (HTTPS)\nansible_httpapi_use_ssl: true\n\n# Disable SSL certificate validation\nansible_httpapi_validate_certs: false\n\n# Credentials\nansible_user: arista\nansible_ssh_pass: arista1c7z\n\n# Global login banner for all switches in topology\nbanner_text: \"This banner came from group_vars/WORKSHOP_FABRIC.YML\"\n</code></pre>  <p>These values will all be inherited by the nodes in the <code>WORKSHOP_FABRIC</code> group defined in our inventory file.</p> <p>We can verify this by running <code>ansible-inventory --host s2-spine1 --yaml</code>.</p> <pre><code>ansible-inventory --host s2-spine1 --yaml\n</code></pre>  Output of 'ansible-inventory --host s1-spine1 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from group_vars/WORKSHOP_FABRIC.YML\n</code></pre>  <p>And there they are! All of the variables we had defined in <code>WORKSHOP_FABRIC.yml</code> are present.</p> <p>If we run this same command but specify <code>s1-leaf3</code>, we'll see some additional and slightly different variables:</p> <pre><code>ansible-inventory --host s1-leaf3 --yaml\n</code></pre>  Output of 'ansible-inventory --host s1-leaf3 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from group_vars/S1.YML\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre>  <p>Whoa! There is certainly more there...and looking at <code>banner_text</code>, we can see that it's different. With <code>group_vars</code>, the closer to the host we get, the higher the precedence of the variable. So, in the case of <code>banner_text</code>, it is defined in <code>WORKSHOP_FABRIC.yml</code> and <code>S1.yml</code>. Because the <code>S1</code> group is closer to the host (<code>s1-leaf3</code>) in this case, the <code>banner_text</code> variable defined in <code>S1.yml</code> take priority.</p>  S1.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/group_vars/S1.yml) <pre><code>mlag_config:\n  domain_id: 1000\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n\nbanner_text: \"This banner came from group_vars/S1.YML\"\n</code></pre>  <p>We'll use the <code>mlag_config</code> stuff later. Let's focus on the <code>banner_text</code> variable for now.</p> <p>Finally, let's take a look at the effective variables on <code>s1-leaf1</code>.</p> <pre><code>ansible-inventory --host s1-leaf1 --yaml\n</code></pre>  Output of 'ansible-inventory --host s1-leaf1 --yaml' <pre><code>ansible_connection: ansible.netcommon.httpapi\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_network_os: arista.eos.eos\nansible_ssh_pass: arista1c7z\nansible_user: arista\nbanner_text: This banner came from host_vars/s1-leaf1.YML\nmlag:\n  enabled: true\n  peer_link_int_1: 1\n  peer_link_int_2: 6\n  side_a: true\nmlag_config:\n  domain_id: 1000\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n</code></pre>  <p>Notice we now have a few more variables, namely the <code>mlag</code> dictionary. We also can see that the <code>banner_text</code> has changed yet again. This time due to the fact that this variable is defined in <code>~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/host_vars/s1-leaf1.yml</code>. This demonstrates that a variable defined in <code>host_vars</code> will take priority over the same variable defined in <code>group_vars</code>.</p>  s1-leaf1.yml (~/project/labfiles/ci-workshops-fundamentals/ansible/inventory/host_vars/s1-leaf1.yml) <pre><code>mlag_config:\n  domain_id: 1000\n  vlan:\n    id: 4094\n    name: mlagpeer\n    trunk_group_name: mlagpeer\n  peer_link_id: 1000\n  side_a:\n    ip: 10.0.0.1/30\n    peer_ip: 10.0.0.2\n  side_b:\n    ip: 10.0.0.2/30\n    peer_ip: 10.0.0.1\n\nbanner_text: \"This banner came from group_vars/S1.YML\"\n</code></pre>  <p>Next, let's use a playbook to deploy our \"Message of the Day\" banner to all of the switches in our lab!</p>","location":"ansible/#variables"},{"title":"Playbooks","text":"<p>Ansible Playbooks are written in YAML and typically consist of four main components:</p> <ul> <li>Plays</li> <li>Tasks</li> <li>Modules</li> <li>Module Parameters</li> </ul> <p>Other items such as variables, conditionals, tags, comments, and more are also available tools for us. But, we will focus on the four main components for our discussion.</p> <p>To do this, we'll review a playbook together. Specifically, the <code>~/project/labfiles/ci-workshops-fundamentals/ansible/playbooks/deploy_banner.yml</code> in our lab environment.</p> <p></p> <p>At the start of our playbook, we have the Play. This is the very root of the playbook. it is where we define the Managed Nodes we'd like to target with this play, as well as the list of tasks we'd like to run on these target hosts.</p> <p>Next, we have the task itself, leveraging the eos_facts module to gather information about the Managed Nodes (devices running Arista's EOS in our topology). In a minute, we'll unpack what a module is behind the scenes.</p> <p>Finally, we have any parameters associated with a module. Some of these parameters may be required, while others may be optional. The best way to learn about a module is to find its documentation out on Ansible Galaxy by searching for it. For example, when we search for <code>eos_</code> on Ansible Galaxy, we get the output below:</p> <p></p> <p>34 Modules, one of which is the <code>eos_banner</code> module, with all its associated documentation!</p> <p>More on Ansible Galaxy in a bit...</p> <p>Let's go ahead and run our playbook! Bonus points if cowsay is enabled.</p> <pre><code>ansible-playbook playbooks/deploy_banner.yml\n</code></pre> <p>Alright! Let's hop into our switches and see what happened...</p>","location":"ansible/#playbooks"},{"title":"s2-spine1","text":"<pre><code>\"This banner came from group_vars/WORKSHOP_FABRIC.yml\"\ns2-spine1#\n</code></pre>","location":"ansible/#s2-spine1"},{"title":"s1-spine1","text":"<pre><code>\"This banner came from group_vars/S1.yml\"\ns1-spine1#\n</code></pre>","location":"ansible/#s1-spine1"},{"title":"s1-leaf1","text":"<pre><code>\"This banner came from host_vars/s1-leaf1.yml\"\ns1-leaf1#\n</code></pre> <p>As expected, each device used whichever <code>banner_text</code> variable was closest to it in the group hierarchy or, in the case of s1-leaf1, was applied via the <code>host_vars</code> file associated with the Node.</p>","location":"ansible/#s1-leaf1"},{"title":"Modules","text":"<p>When we ran our playbook, one of the key components was the module. In our case, this was the eos_banner module.</p> <p>But what is this module doing behind the scenes? Why are modules such a key piece of what makes Ansible much easier to start with than other Automation Frameworks?</p> <p>Modules are an abstraction of the Python code necessary to complete a given task, or tasks, associated with the module. In other words, behind the scenes, modules are just Python code!</p> <p>An example of this can be seen below, specifically for the eos_banner module:</p> <p></p> <p>It's a lot easier to call <code>eos_banner</code> in a playbook than it is to write, in a reusable fashion, all of that Python code! This is why modules are such as big reason Ansible is easy to get started with - the abstraction of the detailed code behind the scenes necessary to get things done.</p> <p>Now, if we feel compelled to dive in and write our own modules, or our own roles/collections/plugins, we can certainly do this. Ansible is very extensible in this manner. When we're finished developing our content, if we want to share it with the world, then we can publish it as a Collection on Ansible Galaxy!.</p>","location":"ansible/#modules"},{"title":"Ansible Galaxy","text":"<p>Earlier, we specified that we were using <code>ansible-core</code> for this workshop. This approach, instead of installing <code>ansible</code>, is becoming increasingly preferred. The reason for this is the efficiency of <code>ansible-core</code>; it is a lightweight minimalist installation of Ansible without any extra modules, roles, plugins, etc., natively included.</p> <p>With <code>ansible-core</code>, we can only grab the modules, roles, plugins, etc. that we need from Ansible Galaxy!.</p> <p></p> <p>For example, if we want to install the Arista EOS Ansible Collection, we can enter the following command in our terminal</p> <pre><code>ansible-galaxy collection install arista.eos\n</code></pre> <p>Once completed, we can validate that the collection has been installed by running the command below:</p> <pre><code>ansible-galaxy collection list\n</code></pre>  Info <p>The <code>ansible-galaxy collection list</code> command is supported in Ansible 2.10+</p>  <p>This will yield output similar to below:</p>  Ansible Galaxy Collection List <pre><code># /home/coder/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 4.1.0\nansible.posix     1.4.0\nansible.utils     2.8.0\narista.avd        3.6.0\narista.cvp        3.4.0\narista.eos        6.0.0\ncommunity.general 6.2.0\n</code></pre>  <p>Note the line in the output <code>/home/coder/.ansible/collections/ansible_collections</code>...how did Ansible know to look here? The answer: The <code>ansible.cfg</code> file!</p> <p>Specifically, this parameter:</p> <pre><code># Path(s) to search for installed Ansible Galaxy Collections\ncollections_paths = ~/.ansible/collections\n</code></pre> <p>Galaxy has thousands of modules, plugins, roles, and more from many community members.</p> <p>For those familiar with Python, we can think of Ansible Galaxy as PyPi, and our Collections as Python modules.</p> <p>Go forth and explore!</p>","location":"ansible/#ansible-galaxy"},{"title":"Ansible Roles","text":"<p></p> <p>Ansible roles allow us to neatly pack all of the tasks, templates, files, etc., that we use to accomplish a task into a nice reusable format. An example of this could be a standardized method for installing NGINX or Apache on a server. Or, as we'll see in our labs, deploying an MLAG domain configuration on Arista switches.</p> <p>Ultimately, Ansible Roles can be seen as a blueprint to automate and standardize our repeatable tasks.</p>","location":"ansible/#ansible-roles"},{"title":"AVD Lab Guide","text":"","location":"avd-lab-guide/"},{"title":"AVD Lab Guide Overview","text":"<p>The AVD Lab Guide is a follow-along set of instructions to deploy a dual data center L2LS fabric design. The data model overview and details can be found here. In the following steps, we will explore updating the data models to add services, ports, and WAN links to our fabrics and test traffic between sites.</p> <p>In this example, the ATD lab is used to create the L2LS Dual Data Center topology below. The IP Network cloud (orange area) is pre-provisioned and is comprised of the border and core nodes in the ATD topology. Our focus will be creating the L2LS AVD data models to build and deploy configurations for Site 1 and Site 2 (blue areas) and connect them to the IP Network.</p> <p></p>","location":"avd-lab-guide/#avd-lab-guide-overview"},{"title":"Host Addresses","text":"Host IP Address     s1-host1 10.10.10.100   s1-host2 10.20.20.100   s2-host1 10.30.30.100   s2-host2 10.40.40.100","location":"avd-lab-guide/#host-addresses"},{"title":"Prepare Lab Environment","text":"","location":"avd-lab-guide/#prepare-lab-environment"},{"title":"STEP #1 - Access the ATD Lab","text":"<p>Connect to your ATD Lab and start the Programmability IDE. Next, create a new Terminal.</p>","location":"avd-lab-guide/#step-1-access-the-atd-lab"},{"title":"STEP #2 - Fork and Clone branch to ATD Lab","text":"<p>An ATD Dual Data Center L2LS data model is posted on GitHub.</p> <ul> <li>Fork this repository to your own GitHub account.</li> <li>Next, clone your forked repo to your ATD lab instance.</li> </ul> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <pre><code>git clone &lt;your copied URL&gt;\n</code></pre> <pre><code>cd ci-workshops-avd\n</code></pre> <p>Configure your global Git settings.</p> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre>","location":"avd-lab-guide/#step-2-fork-and-clone-branch-to-atd-lab"},{"title":"STEP #3 - Update AVD to the latest version","text":"<p>AVD has been pre-installed in your lab environment. However, it may be on an older version. The following steps will update AVD and modules to the latest versions.</p> <pre><code>ansible-galaxy collection install -r requirements.yml\nexport ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1)\npip3 config set global.disable-pip-version-check true\npip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt\n</code></pre>  Important <p>You must run these commands when you start your lab or a new shell (terminal).</p>","location":"avd-lab-guide/#step-3-update-avd-to-the-latest-version"},{"title":"STEP #4 - Setup Lab Password Environment Variable","text":"<p>Each lab comes with a unique password. We set an environment variable called <code>LABPASSPHRASE</code> with the following command. The variable is later used to generate local user passwords and connect to our switches to push configs.</p> <pre><code>export LABPASSPHRASE=`cat /home/coder/.config/code-server/config.yaml| grep \"password:\" | awk '{print $2}'`\n</code></pre> <p>You can view the password is set. This is the same password displayed when you click the link to access your lab.</p> <pre><code>echo $LABPASSPHRASE\n</code></pre>  IMPORTANT <p>You must run this step when you start your lab or a new shell (terminal).</p>","location":"avd-lab-guide/#step-4-setup-lab-password-environment-variable"},{"title":"STEP #5 - Prepare WAN IP Network and Test Hosts","text":"<p>The last step in preparing your lab is to push pre-defined configurations to the WAN IP Network (cloud) and the four hosts used to test traffic. The spines from each site will connect to the WAN IP Network with P2P links. The hosts (two per site) have port-channels to the leaf pairs and are pre-configured with an IP address and route to reach the other hosts.</p> <p>Run the following to push the configs.</p> <pre><code>make preplab\n</code></pre>","location":"avd-lab-guide/#step-5-prepare-wan-ip-network-and-test-hosts"},{"title":"Build and Deploy Dual Data Center L2LS Network","text":"<p>This section will review and update the existing L2LS data model. We will add features to enable VLANs, SVIs, connected endpoints, and P2P links to the WAN IP Network. After the lab, you will have enabled an L2LS dual data center network through automation with AVD. YAML data models and Ansible playbooks will be used to generate EOS CLI configurations and deploy them to each site. We will start by focusing on building out Site 1 and then repeat similar steps for Site 2. Finally, we will enable connectivity to the WAN IP Network to allow traffic to pass between sites.</p>","location":"avd-lab-guide/#build-and-deploy-dual-data-center-l2ls-network"},{"title":"Summary of Steps","text":"<ol> <li>Build and Deploy <code>Site 1</code></li> <li>Build and Deploy <code>Site 2</code></li> <li>Connect sites to WAN IP Network</li> <li>Verify routing</li> <li>Test traffic</li> </ol>","location":"avd-lab-guide/#summary-of-steps"},{"title":"Site 1","text":"","location":"avd-lab-guide/#site-1"},{"title":"STEP #1 - Build and Deploy Initial Fabric","text":"<p>The initial fabric data model key/value pairs have been pre-populated in the following group_vars files in the <code>sites/site_1/group_vars/</code> directory.</p> <ul> <li>SITE1_FABRIC_PORTS.yml</li> <li>SITE1_FABRIC_SERVICES.yml</li> <li>SITE1_FABRIC.yml</li> <li>SITE1_LEAFS.yml</li> <li>SITE1_SPINES.yml</li> </ul> <p>Review these files to understand how they relate to the topology above.</p> <p>At this point, we can build and deploy our initial configurations to the topology.</p> <pre><code>make build-site-1\n</code></pre> <p>AVD creates a separate markdown and EOS configuration file per switch. In addition, you can review the files in the <code>documentation</code> and <code>intended</code> folders per site.</p> <p></p> <p>Now, deploy the configurations to Site 1 switches.</p> <pre><code>make deploy-site-1\n</code></pre> <p>Login to your switches to verify the current configs (<code>show run</code>) match the ones created in <code>intended/configs</code> folder.</p> <p>You can also check the current state for MLAG, VLANs, interfaces, and port-channels.</p> <pre><code>show mlag\n</code></pre> <pre><code>show vlan brief\n</code></pre> <pre><code>show ip interface brief\n</code></pre> <pre><code>show port-channel\n</code></pre> <p>The basic fabric with MLAG peers and port-channels between leaf and spines are now created. Next up, we will add VLAN and SVI services to the fabric.</p>","location":"avd-lab-guide/#step-1-build-and-deploy-initial-fabric"},{"title":"STEP #2 - Add Services to the Fabric","text":"<p>The next step is to add Vlans and SVIs to the fabric. The services data model file <code>SITE1_FABRIC_SERVICES.yml</code> is pre-populated with Vlans and SVIs <code>10</code> and <code>20</code> in the default VRF.</p> <p>Open <code>SITE1_FABRIC_SERVICES.yml</code> and uncomment lines 1-28, then run the build &amp; deploy process again.</p>  Tip <p> In VS Code, you can toggle comments on/off by selecting the text and pressing windows Ctrl + / or mac Cmd + /.</p>  <pre><code>make build-site-1\n</code></pre> <pre><code>make deploy-site-1\n</code></pre> <p>Log into <code>s1-spine1</code> and <code>s1-spine2</code> and verify the SVIs <code>10</code> and <code>20</code> exist.</p> <pre><code>show ip interface brief\n</code></pre> <p>It should look similar to the following:</p> <pre><code>                                           Address\nInterface         IP Address            Status       Protocol            MTU    Owner\n----------------- --------------------- ------------ -------------- ----------- -------\nLoopback0         10.1.252.1/32         up           up                65535\nManagement0       192.168.0.10/24       up           up                 1500\nVlan10            10.10.10.2/24         up           up                 1500\nVlan20            10.20.20.2/24         up           up                 1500\nVlan4093          10.1.254.0/31         up           up                 1500\nVlan4094          10.1.253.0/31         up           up                 1500\n</code></pre> <p>You can verify the recent configuration session was created.</p>  Info <p>When the configuration is applied via a configuration session, EOS will create a \"checkpoint\" of the configuration. This checkpoint is a snapshot of the device's running configuration as it was prior to the configuration session being committed.</p>  <pre><code>show clock\n</code></pre> <pre><code>show configuration sessions detail\n</code></pre> <p>List the recent checkpoints.</p> <pre><code>show config checkpoints\n</code></pre> <p>View the contents of the latest checkpoint file.</p> <pre><code>more checkpoint:&lt; filename &gt;\n</code></pre> <p>See the difference between the running config and the latest checkpoint file.</p>  Tip <p>This will show the differences between the current device configuration and the configuration before we did our <code>make deploy</code> command.</p>  <pre><code>diff checkpoint:&lt; filename &gt; running-config\n</code></pre>","location":"avd-lab-guide/#step-2-add-services-to-the-fabric"},{"title":"STEP #3 - Add Ports for Hosts","text":"<p>Let's configure port-channels to our hosts (<code>s1-host1</code> and <code>s1-host2</code>).</p> <p>Open <code>SITE1_FABRIC_PORTS.yml</code> and uncomment lines 17-45, then run the build &amp; deploy process again.</p> <pre><code>make build-site-1\n</code></pre> <pre><code>make deploy-site-1\n</code></pre> <p>At this point, hosts should be able to ping each other across the fabric.</p> <p>From <code>s1-host1</code>, run a ping to <code>s1-host2</code>.</p> <pre><code>ping 10.20.20.100\n</code></pre> <pre><code>PING 10.20.20.100 (10.20.20.100) 72(100) bytes of data.\n80 bytes from 10.20.20.100: icmp_seq=1 ttl=63 time=30.2 ms\n80 bytes from 10.20.20.100: icmp_seq=2 ttl=63 time=29.5 ms\n80 bytes from 10.20.20.100: icmp_seq=3 ttl=63 time=28.8 ms\n80 bytes from 10.20.20.100: icmp_seq=4 ttl=63 time=24.8 ms\n80 bytes from 10.20.20.100: icmp_seq=5 ttl=63 time=26.2 ms\n</code></pre> <p>Site 1 fabric is now complete.</p>","location":"avd-lab-guide/#step-3-add-ports-for-hosts"},{"title":"Site 2","text":"<p>Repeat the previous three steps for Site 2.</p> <ul> <li>Add Services</li> <li>Add Ports</li> <li>Build and Deploy Configs</li> <li>Verify ping traffic between hosts <code>s2-host1</code> and <code>s2-host2</code></li> </ul> <p>At this point, you should be able to ping between hosts within a site but not between sites. For this, we need to build connectivity to the <code>WAN IP Network</code>. This is covered in the next section.</p>","location":"avd-lab-guide/#site-2"},{"title":"Connect Sites to WAN IP Network","text":"<p>The WAN IP Network is defined by the <code>core_interfaces</code> data model. Full data model documentation is located here.</p> <p>The data model defines P2P links (<code>/31s</code>) on the spines with a stanza per link. See details in the graphic below. Each spine has two links to the WAN IP Network configured on ports <code>Ethernet7</code> and <code>Ethernet8</code>. OSPF is added to these links as well.</p> <p></p>","location":"avd-lab-guide/#connect-sites-to-wan-ip-network"},{"title":"Add P2P Links to WAN IP Network for Site 1 and 2","text":"<p>Add each site's <code>core_interfaces</code> dictionary (shown below) to the bottom of the following files <code>SITE1_FABRIC.yml</code> and <code>SITE2_FABRIC.yml</code></p>","location":"avd-lab-guide/#add-p2p-links-to-wan-ip-network-for-site-1-and-2"},{"title":"Site #1","text":"<p>Add the following code block to the bottom of <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <pre><code>##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>","location":"avd-lab-guide/#site-1_1"},{"title":"Site #2","text":"<p>Add the following code block to the bottom of <code>sites/site_2/group_vars/SITE2_FABRIC.yml</code>.</p> <pre><code>##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.37/31, 10.0.0.36/31 ]\n      nodes: [ s2-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.41/31, 10.0.0.40/31 ]\n      nodes: [ s2-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.39/31, 10.0.0.38/31 ]\n      nodes: [ s2-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.43/31, 10.0.0.42/31 ]\n      nodes: [ s2-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>","location":"avd-lab-guide/#site-2_1"},{"title":"Build and Deploy WAN IP Network connectivity","text":"<pre><code>make build-site-1 build-site-2 deploy-site-1 deploy-site-2\n</code></pre>  Tip <p>Daisy chaining \"Makesies\" is a great way to run a series of tasks with a single CLI command </p>","location":"avd-lab-guide/#build-and-deploy-wan-ip-network-connectivity"},{"title":"Check routes on spine nodes","text":"<p>From the spines, verify that they can see routes to the following networks where the hosts reside.</p> <ul> <li>10.10.10.0/24</li> <li>10.20.20.0/24</li> <li>10.30.30.0/24</li> <li>10.40.40.0/24</li> </ul> <pre><code>show ip route\n</code></pre>","location":"avd-lab-guide/#check-routes-on-spine-nodes"},{"title":"Test traffic between sites","text":"<p>From <code>s1-host1</code> ping both <code>s2-host1</code> &amp; <code>s2-host2</code>.</p> <pre><code># s2-host1\nping 10.30.30.100\n</code></pre> <pre><code># s2-host2\nping 10.40.40.100\n</code></pre>","location":"avd-lab-guide/#test-traffic-between-sites"},{"title":"Congratulations!","text":"<p>You have built a multi-site L2LS network without touching the CLI on a single switch.</p>","location":"avd-lab-guide/#congratulations"},{"title":"Day 2 Operations","text":"<p>Our multi-site L2LS network is working great. But, before too long, it will be time to change our configurations. Lucky for us, that time is today!</p>","location":"avd-lab-guide/#day-2-operations"},{"title":"Cleaning Up","text":"<p>Before going any further, let's ensure we have a clean repo by committing the changes we've made up to this point. The CLI commands below can accomplish this, but the VS Code Source Control GUI can be used as well.</p> <pre><code>git add .\ngit commit -m 'Your message here'\n</code></pre> <p>Next, we'll want to push these changes to our forked repository on GitHub.</p> <pre><code>git push\n</code></pre> <p>If this is our first time pushing to our forked repository, then VS Code will provide us with the following sign-in prompt:</p> <p></p> <p>Choose Allow, and another prompt will come up, showing your unique login code:</p> <p></p> <p>Choose Copy &amp; Continue to GitHub, and another prompt will come up asking if it's ok to open an external website (GitHub).</p> <p></p> <p>Choose Open and then an external site (GitHub) will open, asking for your login code.</p> <p></p> <p>Paste in your login code and choose Continue. You will then be prompted to Authorize VS Code.</p> <p></p> <p>Choose Authorize Visual-Studio-Code, and you should be presented with the coveted Green Check Mark!</p> <p></p> <p>Whew! Alright. Now that we have that complete, let's keep moving...</p>","location":"avd-lab-guide/#cleaning-up"},{"title":"Branching Out","text":"<p>Before jumping in and modifying our files, we'll create a branch named banner-syslog in our forked repository to work on our changes. We can create our branch in multiple ways, but we'll use the <code>git switch</code> command with the <code>-c</code> parameter to create our new branch.</p> <pre><code>git switch -c banner-syslog\n</code></pre> <p>After entering this command, we should see our new branch name reflected in the terminal. It will also be reflected in the status bar in the lower left-hand corner of our VS Code window (you may need to click the refresh icon before this is shown).</p> <p>Now we're ready to start working on our changes .</p>","location":"avd-lab-guide/#branching-out"},{"title":"Login Banner","text":"<p>When we initially deployed our multi-site topology, we should have included a login banner on all our switches. Let's take a look at the AVD documentation site to see what the data model is for this configuration.</p> <p>The banner on all of our switches will be the same. After reviewing the AVD documentation, we know we can accomplish this by defining the <code>banners</code> input variable in our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Login Banner\nbanners:\n  motd: |\n    You shall not pass. Unless you are authorized. Then you shall pass.\n    EOF\n</code></pre>  Yes, that \"EOF\" is important! <p>Ensure the entire code snippet above is copied; including the <code>EOF</code>. This must be present for the configuration to be considered valid</p>  <p>Next, let's build out the configurations and documentation associated with this change.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Please take a minute to review the results of our five lines of YAML. When finished reviewing the changes, let's commit them.</p> <p>As usual, there are a few ways of doing this, but the CLI commands below will get the job done:</p> <pre><code>git add .\ngit commit -m 'add banner'\n</code></pre> <p>So far, so good! Before we publish our branch and create a Pull Request though, we have some more work to do...</p>","location":"avd-lab-guide/#login-banner"},{"title":"Syslog Server","text":"<p>Our next Day 2 change is adding a syslog server configuration to all our switches. Once again, we'll take a look at the AVD documentation site to see the data model associated with the <code>logging</code> input variable.</p> <p>Like our banner operation, the syslog server configuration will be consistent on all our switches. Because of this, we can also put this into our <code>global_vars/global_dc_vars.yml</code> file.</p> <p>Add the code block below to <code>global_vars/global_dc_vars.yml</code>.</p> <pre><code># Syslog\nlogging:\n  vrfs:\n    - name: default\n      source_interface: Management0\n      hosts:\n        - name: 10.200.0.108\n        - name: 10.200.1.108\n</code></pre> <p>Finally, let's build out our configurations.</p> <pre><code>make build-site-1 build-site-2\n</code></pre> <p>Take a minute, using the source control feature in VS Code, to review what has changed as a result of our work.</p> <p>At this point, we have our Banner and Syslog configurations in place. The configurations look good, and we're ready to share this with our team for review. In other words, it's time to publish our branch to the remote origin (our forked repo on GitHub) and create the Pull Request (PR)!</p> <p>There are a few ways to publish the <code>banner-syslog</code> branch to our forked repository. The commands below will accomplish this via the CLI:</p> <pre><code>git add .\ngit commit -m 'add syslog'\ngit push --set-upstream origin banner-syslog\n</code></pre> <p>On our forked repository, let's create the Pull Request.</p> <p>When creating the PR, ensure that the <code>base repository</code> is the main branch of your fork. This can be selected via the dropdown as shown below:</p> <p></p> <p>Take a minute to review the contents of the PR. Assuming all looks good, let's earn the YOLO GitHub badge by approving and merging your PR!</p>  Tip <p>Remember to delete the banner-syslog branch after performing the merge - Keep that repo clean!</p>  <p>Once merged, let's switch back to our <code>main</code> branch and pull down our merged changes.</p> <pre><code>git switch main\ngit pull\n</code></pre> <p>Then, let's delete our now defunct banner-syslog branch.</p> <pre><code>git branch -D banner-syslog\n</code></pre> <p>Finally, let's deploy our changes.</p> <pre><code>make deploy-site-1 deploy-site-2\n</code></pre> <p>Once completed, we should see our banner when logging into any switch. The output of the <code>show logging</code> command should also have our newly defined syslog servers.</p>","location":"avd-lab-guide/#syslog-server"},{"title":"Provisioning new Switches","text":"<p>Our network is gaining popularity, and it's time to add a new Leaf pair into the environment! s1-leaf5 and s1-leaf6 are ready to be provisioned, so let's get to it.</p>","location":"avd-lab-guide/#provisioning-new-switches"},{"title":"Branch Time","text":"<p>Before jumping in, let's create a new branch for our work. We'll call this branch add-leafs.</p> <pre><code>git switch -c add-leafs\n</code></pre> <p>Now that we have our branch created let's get to work!</p>","location":"avd-lab-guide/#branch-time"},{"title":"Inventory Update","text":"<p>First, we'll want to add our new switches, named s1-leaf5 and s1-leaf6, into our inventory file. We'll add them as members of the <code>SITE1_LEAFS</code> group.</p> <p>Add the following two lines under <code>s1-leaf4</code> in <code>sites/site_1/inventory.yml</code>.</p> <pre><code>s1-leaf5:\ns1-leaf6:\n</code></pre> <p>The <code>sites/site_1/inventory.yml</code> file should now look like the example below:</p>  sites/site_1/inventory.yml <pre><code>---\nSITE1:\n  children:\n    CVP:\n      hosts:\n        cvp:\n    SITE1_FABRIC:\n      children:\n        SITE1_SPINES:\n          hosts:\n            s1-spine1:\n            s1-spine2:\n        SITE1_LEAFS:\n          hosts:\n            s1-leaf1:\n            s1-leaf2:\n            s1-leaf3:\n            s1-leaf4:\n            s1-leaf5:\n            s1-leaf6:\n    SITE1_FABRIC_SERVICES:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n    SITE1_FABRIC_PORTS:\n      children:\n        SITE1_SPINES:\n        SITE1_LEAFS:\n</code></pre>  <p>Next, let's add our new Leaf switches into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <p>These new switches will go into RACK3, leverage MLAG for multi-homing, and will have locally connected endpoints in VLANs <code>10</code> and <code>20</code>.</p> <p>Just like the other Leaf switches, interfaces <code>Ethernet2</code> and <code>Ethernet3</code> will be used to connect to the spines.</p> <p>On the spines, interface <code>Ethernet9</code> will be used to connect to s1-leaf5, while <code>Ethernet10</code> will be used to connect to s1-leaf6.</p> <p>Starting at line 64, add the following code block into <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code>.</p> <pre><code>- group: RACK3\n  nodes:\n    - name: s1-leaf5\n      id: 7\n      mgmt_ip: 192.168.0.28/24\n      uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n    - name: s1-leaf6\n      id: 8\n      mgmt_ip: 192.168.0.29/24\n      uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n</code></pre>  Warning <p>Make sure the indentation of <code>RACK3</code> is the same as <code>RACK2</code>, which can be found on line 52</p>  <p>The <code>sites/site_1/group_vars/SITE1_FABRIC.yml</code> file should now look like the example below:</p>  sites/site_1/group_vars/SITE1_FABRIC.yml <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l2ls\n\n# Spine Switches\nl3spine:\n  defaults:\n    platform: cEOS\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.1.252.0/24\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    mlag_peer_l3_ipv4_pool: 10.1.254.0/24\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: SPINES\n      nodes:\n        - name: s1-spine1\n          id: 1\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: RACK1\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 3\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 4\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 5\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 6\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n    - group: RACK3\n      nodes:\n        - name: s1-leaf5\n          id: 7\n          mgmt_ip: 192.168.0.28/24\n          uplink_switch_interfaces: [ Ethernet9, Ethernet9 ]\n        - name: s1-leaf6\n          id: 8\n          mgmt_ip: 192.168.0.29/24\n          uplink_switch_interfaces: [ Ethernet10, Ethernet10 ]\n\n\n##################################################################\n# Underlay Routing Protocol - ran on Spines\n##################################################################\n\nunderlay_routing_protocol: OSPF\n\n##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>   Tip <p>Notice how we did not specify a <code>filter</code> or <code>tags</code> under <code>RACK3</code>. If the <code>filter</code> parameter is not defined, all VLANs/SVIs/VRFs will be provisioned on the switch. In our case, this means that VLANs <code>10</code> and <code>20</code> will both be created on our new Leaf switches. However, since they are <code>leaf</code> node types, no SVIs will be created.</p>  <p>Next - Let's build the configuration!</p> <pre><code>make build-site-1\n</code></pre>  Important <p>Interfaces <code>Ethernet9</code> and <code>Ethernet10</code> do not exist on the Spines. Because of this, we will not run a deploy command since it would fail.</p>  <p>Please take a moment and review the results of our changes via the source control functionality in VS Code.</p> <p>Finally, we'll commit our changes and publish our branch. Again, we can use the VS Code Source Control GUI for this, or via the CLI using the commands below:</p> <pre><code>git add .\ngit commit -m 'add leafs'\ngit push --set-upstream origin add-leafs\n</code></pre>","location":"avd-lab-guide/#inventory-update"},{"title":"Backing out changes","text":"<p>Ruh Roh. As it turns out, we should have added these leaf switches to an entirely new site. Oops! No worries, because we used our add-leafs branch, we can switch back to our main branch and then delete our local copy of the add-leafs branch. No harm or confusion related to this change ever hit the main branch!</p> <pre><code>git switch main\ngit branch -D add-leafs\n</code></pre> <p>Finally, we can go out to our forked copy of the repository and delete the add-leafs branch.</p> <p>All set!</p>","location":"avd-lab-guide/#backing-out-changes"},{"title":"Arista CI Workshop","text":"<p>This workshop will leverage open-source tools to build a network CI pipeline for configuration development, deployment, documentation, and validation. In addition, the open-source tooling enables us to manage our network environment as code.</p> <p>This section will cover the following:</p> <ul> <li>Arista Validated Designs (AVD) Ansible Collection</li> <li>Network Data Models</li> <li>Initial Deployment (Day 0 Provisioning)</li> <li>Ongoing Operations (Day 2 and Beyond)</li> <li>Validation and Troubleshooting</li> <li>Enhancing our CI/CD Pipelines with GitHub Actions</li> </ul> <p>Each attendee will receive a dedicated virtual lab environment with Git, VS Code, and Ansible installed and ready to use.</p> <p>Attendees will need the following:</p> <ul> <li>A laptop</li> <li>An account on GitHub</li> <li>Familiarity with the concepts and tools covered in the previous Automation Fundamentals workshop (Git, VS Code, Jinja/YAML, Ansible)</li> </ul>","location":"avd/"},{"title":"Installation Requirements","text":"","location":"avd/#installation-requirements"},{"title":"ATD Environment","text":"<p>The ATD lab environment was provisioned with Ansible and Git. First, however, we must update AVD and the required modules to the latest version. The following commands will install AVD and the needed modules.</p> <pre><code>ansible-galaxy collection install arista.avd arista.cvp --force\nexport ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1)\npip3 config set global.disable-pip-version-check true\npip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt\n</code></pre>  Note <p>IMPORTANT: The above steps must be run each time you start your lab.</p>","location":"avd/#atd-environment"},{"title":"Other Environments","text":"<p>Install AVD and required modules - Installation guide found here.</p>","location":"avd/#other-environments"},{"title":"Lab Topology Overview","text":"<p>Throughout this section, we will use the following dual data center topology. Click on the image to zoom in for details.</p> <p></p>","location":"avd/#lab-topology-overview"},{"title":"Basic EOS Switch Configuration","text":"<p>Basic connectivity between the Ansible controller host and the switches must be established before Ansible can be used to deploy configurations. The following should be configured on all switches:</p> <ul> <li>Switch Hostname</li> <li>IP enabled interface</li> <li>Username and Password defined</li> <li>Management eAPI enabled</li> </ul>  Info <p>In the ATD environment, cEOS virtual switches use <code>Management0</code> in the default VRF. When using actual hardware or vEOS switches, <code>Management1</code> is used. The included basic switch configurations may need to be adjusted for your environment.</p>  <p>Below is an example basic configuration file for s1-spine1:</p> <pre><code>!\nno aaa root\n!\nusername admin privilege 15 role network-admin secret sha512 $6$eucN5ngreuExDgwS$xnD7T8jO..GBDX0DUlp.hn.W7yW94xTjSanqgaQGBzPIhDAsyAl9N4oScHvOMvf07uVBFI4mKMxwdVEUVKgY/.\n!\nhostname s1-spine1\n!\nmanagement api http-commands\n   no shutdown\n!\ninterface Management0\n   ip address 192.168.0.10/24\n!\nip routing\n!\nip route vrf MGMT 0.0.0.0/0 192.168.0.1\n!\n</code></pre>","location":"avd/#basic-eos-switch-configuration"},{"title":"Ansible Inventory","text":"<p>Our lab L2LS topology contains two sites, <code>Site 1</code> and <code>Site 2</code>. We need to create the Ansible inventory for each site. We have created two separate directories for each site under the <code>sites</code> sub-directory in our repo.</p> <pre><code>\u251c\u2500\u2500 sites/\n  \u251c\u2500\u2500 site_1/\n  \u251c\u2500\u2500 site_2/\n</code></pre> <p>The following is a graphical representation of the Ansible inventory groups and naming scheme used for <code>Site 1</code> in this example. This is replicated for <code>Site 2</code>.</p> <p></p>","location":"avd/#ansible-inventory"},{"title":"AVD Fabric Variables","text":"<p>To apply AVD variables to the nodes in the fabric, we make use of Ansible group_vars. How and where you define the variables is your choice. The group_vars table below is one example of AVD fabric variables for <code>Site 1</code>.</p>    group_vars/ Description     SITE1_FABRIC.yml Fabric, Topology, and Device settings   SITE1_SPINES.yml Device type for Spines   SITE1_LEAFS.yml Device type for Leafs   SITE1_NETWORK_SERVICES.yml VLANs, VRFs, SVIs   SITE1_NETWORK_PORTS.yml Port Profiles and Connected Endpoint settings    <p>Each group_vars file is listed in the following tabs.</p> SITE1_FABRICSITE1_SPINESSITE1_LEAFSSITE1_NETWORK_SERVICESSITE1_NETWORK_PORTS   <p>At the Fabric level (SITE1_FABRIC), the following variables are defined in group_vars/SITE1_FABRIC.yml. The fabric name, design type (l2ls), node type defaults, interface links, and core interface P2P links are defined at this level. Other variables you must supply include:</p> <ul> <li>spanning_tree_mode</li> <li>spanning_tree_priority</li> <li>mlag_peer_ipv4_pool</li> </ul> <p>The l3spine node will need these additional variables set.</p> <ul> <li>loopback_ipv4_pool</li> <li>mlag_peer_l3_ipv4_pool</li> <li>virtual_router_mac_address</li> </ul> <p>Variables applied under the node key type (l3spine/leaf) defaults section are inherited by nodes under each type. These variables may be overwritten under the node itself.</p> <p>The spine interface used by a particular leaf is defined from the leaf's perspective with a variable called <code>uplink_switch_interfaces</code>. For example, s1-leaf1 has a unique variable <code>uplink_switch_interfaces: [Ethernet2, Ethernet2]</code> defined. This means that s1-leaf1 is connected to <code>s1-spine1</code> Ethernet2 and <code>s1-spine2</code> Ethernet2, respectively.</p> <pre><code>---\nfabric_name: SITE1_FABRIC\n\n# Set Design Type to L2ls\ndesign:\n  type: l2ls\n\n# Spine Switches\nl3spine:\n  defaults:\n    platform: cEOS\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.1.252.0/24\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    mlag_peer_l3_ipv4_pool: 10.1.254.0/24\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: SPINES\n      nodes:\n        - name: s1-spine1\n          id: 1\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n# Leaf Switches\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    - group: RACK1\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 3\n          mgmt_ip: 192.168.0.12/24\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 4\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 5\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 6\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n\n##################################################################\n# Underlay Routing Protocol - ran on Spines\n##################################################################\n\nunderlay_routing_protocol: OSPF\n\n##################################################################\n# WAN/Core Edge Links\n##################################################################\n\ncore_interfaces:\n  p2p_links:\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre>   <p>In an L2LS design, there are two types of spine nodes: <code>spine</code> and <code>l3spine</code>. In AVD, the node type defines the functionality and the EOS CLI configuration to be generated. For our L2LS topology, we will use node type <code>l3spine</code> to include SVI functionality.</p> <pre><code>---\ntype: l3spine\n</code></pre>   <p>In an L2LS design, we have one type of leaf node: <code>leaf</code>. This will provide L2 functionality to the leaf nodes.</p> <pre><code>---\ntype: leaf\n</code></pre>   <p>You add VLANs to the Fabric by updating the group_vars/SITE1_NETWORK_SERVICES.yml. Each VLAN will be given a name and a list of tags. The tags filter the VLAN to specific Leaf Pairs. These variables are applied to spine and leaf nodes since they are a part of this nested group.</p> <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 10\n            name: 'Ten'\n            tags: [ \"Web\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n</code></pre>   <p>Our fabric would only be complete by connecting some devices to it. We define connected endpoints and port profiles in group_vars/SITE1_NETWORKS_PORTS.yml. Each endpoint adapter defines which switch port and port profile to use. Our lab has two hosts connected to the <code>site 1</code> fabric. The connected endpoints keys are used for logical separation and apply to interface descriptions. These variables are applied to the spine and leaf nodes since they are a part of this nested inventory group.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>","location":"avd/#avd-fabric-variables"},{"title":"Global Variables","text":"<p>In a multi-site environment, some variables must be applied to all sites. They include AAA, Local Users, NTP, Syslog, DNS, and TerminAttr. Instead of updating these same variables in multiple inventory group_vars, we can use a single global variable file and import the variables at playbook runtime. This allows us to make a single change applied to all sites.</p> <p>For example, in our lab, we use a global variable file <code>global_vars/global_dc-vars.yml</code>.</p> <p>AVD provides a <code>global_vars</code> plugin that enables the use of global variables.</p> <p>The <code>global_vars</code> plugin must be enabled in the <code>ansible.cfg</code> file as shown below:</p> <pre><code>#enable global vars\nvars_plugins_enabled = arista.avd.global_vars, host_group_vars\n\n#define global vars path\n[vars_global_vars]\npaths = ../../global_vars\n</code></pre>  Info <p>If a folder is used as in the example above, all files in the folder will be parsed in alphabetical order.</p>","location":"avd/#global-variables"},{"title":"Example Global Vars File","text":"global_vars/global_dc_vars.yml <pre><code>---\n\n# Credentials for CVP and EOS Switches\nansible_user: arista\nansible_password: \"{{ lookup('env', 'LABPASSPHRASE') }}\"\nansible_network_os: arista.eos.eos\n# Configure privilege escalation\nansible_become: true\nansible_become_method: enable\n# HTTPAPI configuration\nansible_connection: httpapi\nansible_httpapi_port: 443\nansible_httpapi_use_ssl: true\nansible_httpapi_validate_certs: false\nansible_python_interpreter: $(which python3)\navd_data_conversion_mode: error\navd_data_validation_mode: error\n\n# Local Users\nlocal_users:\n  - name: arista\n    privilege: 15\n    role: network-admin\n    sha512_password: \"{{ ansible_password | password_hash(salt='workshop') }}\"\n\n# AAA\naaa_authorization:\n  exec:\n    default: local\n\n# OOB Management network default gateway.\nmgmt_gateway: 192.168.0.1\nmgmt_interface: Management0\nmgmt_interface_vrf: default\n\n# NTP Servers IP or DNS name, first NTP server will be preferred, and sourced from Management VRF\nntp:\n  servers:\n    - name: 192.168.0.1\n      iburst: true\n      local_interface: Management0\n\n# Domain/DNS\ndns_domain: atd.lab\n\n# TerminAttr\ndaemon_terminattr:\n  # Address of the gRPC server on CloudVision\n  # TCP 9910 is used on on-prem\n  # TCP 443 is used on CV as a Service\n  cvaddrs: # For single cluster\n    - 192.168.0.5:9910\n  # Authentication scheme used to connect to CloudVision\n  cvauth:\n    method: token\n    token_file: \"/tmp/token\"\n  # Exclude paths from Sysdb on the ingest side\n  ingestexclude: /Sysdb/cell/1/agent,/Sysdb/cell/2/agent\n  # Exclude paths from the shared memory table\n  smashexcludes: ale,flexCounter,hardware,kni,pulse,strata\n\n# Point to Point Links MTU Override for Lab\np2p_uplinks_mtu: 1500\n\n# CVP node variables\ncv_collection: v3\nexecute_tasks: true\n</code></pre>","location":"avd/#example-global-vars-file"},{"title":"Data Models","text":"<p>AVD provides a network-wide data model and is typically broken into multiple group_vars files to simplify and categorize variables with their respective functions. We break the data model into three categories: topology, services, and ports.</p>","location":"avd/#data-models"},{"title":"Fabric Topology","text":"<p>The physical fabric topology is defined by providing interface links between the spine and leaf nodes. The <code>group_vars/SITE1_FABRIC.yml</code> file defines this portion of the data model. In our lab, the spines provide layer 3 routing of SVIs and P2P links using a node type called <code>l3spines</code>. The leaf nodes are purely layer 2 and use node type <code>leaf</code>. An AVD L2LS design type provides three node type keys: l3\u00a0spine, spine, and leaf. AVD Node Type documentation can be found here.</p>","location":"avd/#fabric-topology"},{"title":"Spine and Leaf Nodes","text":"<p>The example data model below defines each site's spine and leaf nodes for each site. Refer to the inline comments for variable definitions. Under each node_type_key you have key/value pairs for defaults, node_groups, and nodes. Note that key/value pairs may be overwritten with the following descending order of precedence. The key/value closest to the node will be used.</p> <p> <ol> <li>defaults</li> <li>node_groups</li> <li>nodes</li> </ol> <pre><code>################################\n# Spine Switches\n################################\n\n# node_type_key for providing L3 services\nl3spine:\n  # default key/values for all l3spine nodes\n  defaults:\n    # Platform dependent default settings for mlag timers, tcam profiles, LANZ, management interface\n    platform: cEOS\n    # Spanning Tree\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 4096\n    # Loopback0 pool, used in conjunction with value of `id:` under each node\n    loopback_ipv4_pool: 10.1.252.0/24\n    # IP Pool for MLAG peer link\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    # IP Pool for L3 peering over the MLAG peer link\n    mlag_peer_l3_ipv4_pool: 10.1.254.0/24\n    # Virtual MAC address used in vARP\n    virtual_router_mac_address: 00:1c:73:00:dc:01\n    # Default MLAG interfaces between spine nodes\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  # keyword for node groups, two nodes within a node group will form an MLAG pair\n  node_groups:\n    # User-defined node group name\n    - group: SPINES\n      # key word for nodes\n      nodes:\n        - name: s1-spine1\n          # unique identifier used for IP address calculations\n          id: 1\n          # Management address assigned to the defined management interface\n          mgmt_ip: 192.168.0.10/24\n        - name: s1-spine2\n          id: 2\n          mgmt_ip: 192.168.0.11/24\n\n################################\n# Leaf Switches\n################################\nleaf:\n  defaults:\n    platform: cEOS\n    mlag_peer_ipv4_pool: 10.1.253.0/24\n    spanning_tree_mode: mstp\n    spanning_tree_priority: 16384\n    # Default uplink switches from leaf perspective\n    uplink_switches: [ s1-spine1, s1-spine2 ]\n    # Default uplink interfaces on leaf nodes connecting to the spines\n    uplink_interfaces: [ Ethernet2, Ethernet3 ]\n    # Default leaf MLAG interfaces\n    mlag_interfaces: [ Ethernet1, Ethernet6 ]\n  node_groups:\n    # User-defined node group name\n    - group: RACK1\n      # Filter which Vlans will be applied to the node_group, comma-separated tags supported\n      # Tags for each Vlan are defined in the SITE1_FABRIC_SERVICES.yml\n      filter:\n        tags: [ \"Web\" ]\n      nodes:\n        - name: s1-leaf1\n          id: 3\n          mgmt_ip: 192.168.0.12/24\n          # Define which interface is configured on the uplink switch\n          # In this example s1-leaf1 connects to [ s1-spine1, s1-spine2 ]\n          # on the following ports. This will be unique to each leaf\n          uplink_switch_interfaces: [ Ethernet2, Ethernet2 ]\n        - name: s1-leaf2\n          id: 4\n          mgmt_ip: 192.168.0.13/24\n          uplink_switch_interfaces: [ Ethernet3, Ethernet3 ]\n    - group: RACK2\n      filter:\n        tags: [ \"App\" ]\n      nodes:\n        - name: s1-leaf3\n          id: 5\n          mgmt_ip: 192.168.0.14/24\n          uplink_switch_interfaces: [ Ethernet4, Ethernet4 ]\n        - name: s1-leaf4\n          id: 6\n          mgmt_ip: 192.168.0.15/24\n          uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]\n</code></pre>","location":"avd/#spine-and-leaf-nodes"},{"title":"Core Interfaces","text":"<p>Inside the same group_vars file, we define how each site is linked to the Core IP Network using point-to-point L3 links on the spine nodes. In our example, OSPF will be used to share routes between sites across the IP Network. The <code>core_interfaces</code> data model for <code>Site 1</code> follows.</p> <pre><code>core_interfaces:\n  p2p_links:\n    # s1-spine1 Ethernet7 to WANCORE\n    - ip: [ 10.0.0.29/31, 10.0.0.28/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine1 Ethernet8 to WANCORE\n    - ip: [ 10.0.0.33/31, 10.0.0.32/31 ]\n      nodes: [ s1-spine1, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine2 Ethernet7 to WANCORE\n    - ip: [ 10.0.0.31/31, 10.0.0.30/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet7, Ethernet2 ]\n      include_in_underlay_protocol: true\n\n    # s1-spine2 Ethernet8 to WANCORE\n    - ip: [ 10.0.0.35/31, 10.0.0.34/31 ]\n      nodes: [ s1-spine2, WANCORE ]\n      interfaces: [ Ethernet8, Ethernet2 ]\n      include_in_underlay_protocol: true\n</code></pre> <p>The following diagram shows the P2P links to the IP Network from all four spine nodes. The WAN IP Network is pre-configured in our lab with /31's running OSPF in area 0.0.0.0. The core_interfaces for the spines in <code>Site 1</code> and <code>Site 2</code> are configured and deployed with AVD.</p> <p></p>","location":"avd/#core-interfaces"},{"title":"Fabric Services","text":"<p>Fabric Services, such as VLANs, SVIs, and VRFs, are defined in this section. The following Site 1 example defines VLANSs and SVIs for VLANs <code>10</code> and <code>20</code> in the default VRF. Additional VRF definitions can also be applied.</p> <pre><code>---\ntenants:\n  # User-defined Tenant/Fabric name\n  - name: MY_FABRIC\n    # key-word\n    vrfs:\n      # Default VRF\n      - name: default\n        # key-word\n        svis:\n          # Vlan ID\n          - id: 10\n            # Vlan Name\n            name: 'Ten'\n            # Tag assigned to Vlan. Used as a filter by each node_group\n            tags: [ \"Web\" ]\n            enabled: true\n            # SVI Virtual ARP address, used along with pre-defined virtual_router_mac_address\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            # Which nodes to apply physical SVI address\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n</code></pre>","location":"avd/#fabric-services"},{"title":"Fabric Ports","text":"<p>The Fabric must define ports for southbound interfaces toward connected endpoints such as servers, appliances, firewalls, and other networking devices in the data center. This section uses port profiles and connected endpoints called <code>servers</code>. Documentation for port_profiles and connected endpoints are available to see all the options available.</p> <p>The following data model defined two port profiles: PP-VLAN10 and PP-VLAN20. They define an access port profile for VLAN <code>10</code> and <code>20</code>, respectively. In addition, two server endpoints (s1-host1 and s1-host2) are created to use these port profiles. There are optional and required fields. The optional fields are used for port descriptions in the EOS intended configurations.</p> <pre><code>---\nport_profiles:\n\n  - profile: PP-VLAN10\n    mode: \"access\"\n    vlans: \"10\"\n    spanning_tree_portfast: edge\n  - profile: PP-VLAN20\n    mode: \"access\"\n    vlans: \"20\"\n    spanning_tree_portfast: edge\n\n###########################################################\n# ---------------- Endpoint Connectivity ---------------- #\n###########################################################\n\nservers:\n\n# --------------------------------------------------------#\n# Site1 RACK1 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host1                                      # Server name\n    rack: RACK1                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf1, s1-leaf2 ]                # Switch to connect server (required)\n        profile: PP-VLAN10                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n\n# --------------------------------------------------------#\n# Site1 RACK2 Endpoints\n# --------------------------------------------------------#\n\n  - name: s1-host2                                      # Server name\n    rack: RACK2                                         # Informational RACK (optional)\n    adapters:\n      - endpoint_ports: [ eth1, eth2 ]                  # Server port to connect (optional)\n        switch_ports: [ Ethernet4, Ethernet4 ]          # Switch port to connect server (required)\n        switches: [ s1-leaf3, s1-leaf4 ]                # Switch to connect server (required)\n        profile: PP-VLAN20                              # Port profile to apply (required)\n        port_channel:\n          mode: active\n</code></pre>","location":"avd/#fabric-ports"},{"title":"The Playbooks","text":"<p>Two playbooks, <code>build.yml</code> and <code>deploy.yml</code> are used in our lab. Expand the tabs below to reveal the content.</p>  build.yml Playbook <pre><code>---\n- name: Build Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Generate Structured Variables per Device\n      import_role:\n        name: arista.avd.eos_designs\n\n    - name: Generate Intended Config and Documentation\n      import_role:\n        name: arista.avd.eos_cli_config_gen\n</code></pre>   deploy.yml Playbook <pre><code>---\n- name: Deploy Switch configuration\n  hosts: \"{{ target_hosts }}\"\n  gather_facts: false\n\n  tasks:\n\n    - name: Deploy Configuration to Device\n      import_role:\n        name: arista.avd.eos_config_deploy_eapi\n</code></pre>  <p>To make our lives easier, we use a <code>Makefile</code> to create aliases to run the playbooks and provide the needed options. This eliminates mistakes and typing long commands.</p>  Makefile <pre><code>.PHONY: help\nhelp: ## Display help message\n    @grep -E '^[0-9a-zA-Z_-]+\\.*[0-9a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\n########################################################\n# Site 1\n########################################################\n\n.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n# ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\" -e \"@global_vars/global_dc_vars.yml\"\n\n.PHONY: deploy-site-1\ndeploy-site-1: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n\n########################################################\n# Site 2\n########################################################\n\n.PHONY: build-site-2\nbuild-site-2: ## Build Configs\n    ansible-playbook playbooks/build.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n\n.PHONY: deploy-site-2\ndeploy-site-2: ## Deploy Configs via eAPI\n    ansible-playbook playbooks/deploy.yml -i sites/site_2/inventory.yml -e \"target_hosts=SITE2_FABRIC\"\n</code></pre>  <p>For example, if we wanted to run a playbook to build configs for Site 1, we could enter the following command.</p> <pre><code>ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Thankfully, a convenient way to simplify the above command is with a Makefile entry like the one below.</p> <pre><code>.PHONY: build-site-1\nbuild-site-1: ## Build Configs\n  ansible-playbook playbooks/build.yml -i sites/site_1/inventory.yml -e \"target_hosts=SITE1_FABRIC\"\n</code></pre> <p>Now, you can type the following to issue the same ansible-playbook command.</p> <pre><code>make build-site-1\n</code></pre> <p>In the upcoming lab, we will use the following <code>make</code> commands several times. First, review the above <code>Makefile</code> to see what each entry does. Then, try building some custom entries.</p> <p>Build configurations</p> <pre><code># Build configs for Site 1\nmake build-site-1\n\n# Build configs for Site 2\nmake build-site-2\n</code></pre> <p>Deploy configurations</p> <pre><code># Deploy configs for Site 1\nmake deploy-site-1\n\n# Deploy configs for Site 2\nmake deploy-site-2\n</code></pre>","location":"avd/#the-playbooks"},{"title":"Next Steps","text":"<p>Continue to Lab Guide</p>","location":"avd/#next-steps"},{"title":"CI/CD","text":"<p>This section walks you through an example CI/CD pipeline leveraging GitHub Actions, Arista Validated Designs (AVD), and the Arista CloudVision Platform (CVP). In addition, the lab leverages the Arista Test Drive (ATD) solution to give you a pre-built environment to get started quickly. This section assumes readers have completed the AVD workshop within their ATD environment.</p> <p>Readers should be familiar with the following concepts.</p> <ul> <li>Git</li> <li>VS Code</li> <li>Jinja &amp; YAML</li> <li>Ansible</li> </ul>","location":"cicd-basics/"},{"title":"The topology","text":"<p>Throughout this section, we will use the following dual data center topology. Click on the image to zoom in for details.</p> <p></p>","location":"cicd-basics/#the-topology"},{"title":"Getting started","text":"<p>This repository leverages the dual data center (DC) ATD. If you are not leveraging the ATD, you may still leverage this repository for a similar deployment. Please note that some updates may have to be made for the reachability of nodes and CloudVision (CVP) instances. This example was created with Ansible AVD version <code>4.1</code>.</p>","location":"cicd-basics/#getting-started"},{"title":"Local installation","text":"<p>If running outside of the ATD interactive developer environment (IDE), you must install the base requirements.</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate\nansible-galaxy collection install -r requirements.yml\nexport ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1)\npip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt\n</code></pre>","location":"cicd-basics/#local-installation"},{"title":"ATD programmability IDE installation","text":"<p>You can ensure the appropriate AVD version is installed by running the following command.</p> <pre><code>ansible-galaxy collection list\n</code></pre> <pre><code>\u279c  ci-workshops-avd git:(main) ansible-galaxy collection list\n\n# /home/coder/.ansible/collections/ansible_collections\nCollection        Version\n----------------- -------\nansible.netcommon 4.1.0\nansible.posix     1.4.0\nansible.utils     2.8.0\narista.avd        4.1.0\narista.cvp        3.6.1\narista.eos        6.0.0\ncommunity.general 6.2.0\n\u279c  ci-workshops-avd git:(main)\n</code></pre> <p>If AVD version <code>4.0.0</code> or greater is not present, please upgrade to the latest stable version.</p> <pre><code>ansible-galaxy collection install -r requirements.yml\nexport ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1)\npip3 config set global.disable-pip-version-check true\npip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt\n</code></pre>","location":"cicd-basics/#atd-programmability-ide-installation"},{"title":"Fork repository","text":"<p>You will be creating your own CI/CD pipeline in this workflow. Log in to your GitHub account and fork the <code>ci-workshops-avd</code> repository to get started.</p>  <p>Note</p> <p>You can skip this step if the repository was forked during the AVD workshop.</p>  <p></p> <p></p>","location":"cicd-basics/#fork-repository"},{"title":"Enable GitHub actions","text":"<ol> <li>Go to Actions</li> <li>Click <code>I understand my workflows, go ahead and enable them</code></li> </ol> <p></p>","location":"cicd-basics/#enable-github-actions"},{"title":"Set GitHub secret","text":"<p>You will need to set one secret in your newly forked GitHub repository.</p> <ol> <li>Go to <code>Settings</code></li> <li>Click <code>Secrets and variables</code></li> <li>Click <code>Actions</code></li> <li> <p>Click <code>New repository secret</code></p> <p></p> </li> <li> <p>Enter the secret as follows</p> <ul> <li>Name: LABPASSPHRASE</li> <li> <p>Secret: Listed in ATD lab topology</p> <p> </p> </li> </ul> </li> <li> <p>Click <code>Add secret</code></p> </li> </ol>  <p>Note</p> <p>Our workflow uses this secret to authenticate with our CVP instance.</p>","location":"cicd-basics/#set-github-secret"},{"title":"Update local CVP variables","text":"<p>Every user will get a unique CVP instance deployed. There are two updates required.</p> <ol> <li> <p>Add the <code>ansible_host</code> variable under the <code>cvp</code> host in the <code>/home/coder/project/labfiles/ci-workshops-avd/sites/site_1/inventory.yml</code> file. The domain name can be located at the top of your ATD lab environment.</p> <pre><code>---\nSITE1:\n  children:\n    CVP:\n      hosts:\n          cvp:\n            ansible_host: &lt;atd-topo12345.topo.testdrive.arista.com&gt;\n   ...\n</code></pre> </li> <li> <p>Add the <code>ansible_host</code> variable under the <code>cvp</code> host in the <code>/home/coder/project/labfiles/ci-workshops-avd/sites/site_2/inventory.yml</code> file.</p> <pre><code>---\nSITE2:\n  children:\n    CVP:\n      hosts:\n          cvp:\n            ansible_host: &lt;atd-topo12345.topo.testdrive.arista.com&gt;\n   ...\n</code></pre> </li> </ol>  <p>Note</p> <p>These will be the same value. Make sure to remove any prefix like <code>https://</code> or anything after <code>.com</code></p>","location":"cicd-basics/#update-local-cvp-variables"},{"title":"Configure global Git settings and sync","text":"<ol> <li>From the IDE terminal, run the following:<pre><code>cd /home/coder/project/labfiles/ci-workshops-avd\n</code></pre> <pre><code>git config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code>git config --global user.email \"name@example.com\"\n</code></pre> <pre><code>git add .\ngit commit -m \"Syncing with remote\"\ngit push\n</code></pre> </li> </ol>  <p>Note</p> <p>If the Git <code>user.name</code> and <code>user.email</code> are set, they may be skipped. You can check this by running the <code>git config --list</code> command. You will get a notification to sign in to GitHub. Follow the prompts.</p>","location":"cicd-basics/#configure-global-git-settings-and-sync"},{"title":"Create a new branch","text":"<p>In a moment, we will be deploying changes to our environment. In reality, updates to a code repository would be done from a development or feature branch. We will follow this same workflow.</p>  <p>Note</p> <p>This example will use the branch name <code>dc-updates</code>. If you use a different branch name, update the upcoming examples appropriately.</p>  <pre><code>git checkout -b dc-updates\n</code></pre>","location":"cicd-basics/#create-a-new-branch"},{"title":"GitHub Actions","text":"<p>GitHub Actions is a CI/CD platform within GitHub. We can leverage GitHub Actions to create automated workflows within our repository. These workflows can be as simple as notifying appropriate reviewers of a change and automating the entire release of an application or network infrastructure.</p>","location":"cicd-basics/#github-actions"},{"title":"Workflow files","text":"<p>GitHub actions are defined by separate files (<code>dev.yml</code> and <code>prod.yml</code>) within our code repository's <code>.github/workflows</code> directory.</p> <p>At the highest level of our workflow file, we set the <code>name</code> of the workflow. This version of our workflow file represents any pushes that do not go to the main branch. For example, we would like our test or development workflow to start whenever we push or change any branches not named main. We can control this by setting the <code>on.push.branches-ignore</code> variable to main.</p> <pre><code># dev.yml\nname: Test the upcoming changes\n\non:\n  push:\n    branches-ignore:\n      - main\n...\n</code></pre> <p>In the next portion of the workflow file, we define a dictionary of <code>jobs</code>. For this example, we will only use one job with multiple steps. We set the ATD credential as an environment variable that will be available for our future steps. The <code>timeout-minutes</code> variable is optional and only included to ensure we remove any long-running workflows. This workflow should come nowhere near the 15-minute mark. Any more than that, and it should signal to us that there is a problem in the workflow. We can see the <code>runs-on</code> key at the end of this code block. This workflow uses the <code>ubuntu-latest</code> flavor, but other options are available. For example, we can use a Windows, Ubuntu, or macOS runner (machines that execute jobs in a GitHub Actions workflow).</p> <pre><code>...\non:\n  push:\n    branches-ignore:\n      - main\n\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n...\n</code></pre> <p>Now that we have defined our <code>dev</code> job, we must define what <code>steps</code> will run within this workflow. For this portion, we have the first and second steps in the workflow. The initial step, \"Hi\" is only used to validate an operational workflow and is not required. Next, the <code>actions/checkout</code> action will check out your repository to make the repository accessible in the workflow. Future workflow steps will then be able to use the relevant repository information to run tasks like building a new application or deploying the latest state of a network.</p> <pre><code>...\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v3\n...\n</code></pre>","location":"cicd-basics/#workflow-files"},{"title":"pre-commit","text":"<p>To get started with pre-commit, run the following commands in your ATD IDE terminal.</p> <pre><code>pip3 install pre-commit\npre-commit install\n</code></pre> <p>We will leverage pre-commit in our local development workflow and within the pipeline. pre-commit works by running automated checks on Git repositories manually or whenever a Git commit is run. For example, if we wanted all of our YAML files to have a similar structure or follow specific guidelines, we could use a pre-commit \"check-yaml\" hook. Please note this is just a sample of what pre-commit can do. For a list of hooks, check out their official list. The code block below references the pre-commit configuration file used in our repository.</p> <p>In pre-commit, we define our jobs under a <code>repos</code> key. This first repo step points to the built-in hooks provided by the pre-commit team. Please note you can use hooks from other organizations. In our case, the checks are fairly simplistic. The first hook checks to ensure our files have no trailing whitespace. The next hook, <code>end-of-file-fixer</code>, ensures every file is empty or ends with one new line. Next, the check YAML hook validates any YAML file in our repository can be loaded as valid YAML syntax. Below is our workflow example leveraging the pre-commit action. This action will read the <code>.pre-commit-config.yaml</code> file in the root of our repository. The <code>files</code> key only checks files within specific directories.</p> <pre><code># .pre-commit-config.yaml\n# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n\n      - id: end-of-file-fixer\n        exclude_types: [svg, json]\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n\n      - id: check-yaml\n        files: sites/site_1/group_vars/|sites/site_2/group_vars/\n</code></pre> <p>Finally, the setup Python and install requirements action above the pre-commit step installs Python dependencies in this workflow.</p> <pre><code>...\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v3\n\n      - name: Install Python requirements\n        run: pip3 install requirements.txt\n\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n...\n</code></pre>","location":"cicd-basics/#pre-commit"},{"title":"pre-commit example","text":"<p>We can look at the benefits of pre-commit by introducing three errors in a group_vars file. This example will use the <code>sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml</code> file. Under VLAN 20, we can add extra whitespace after any entry, extra newlines, and move the <code>s1-spine2</code> key under the <code>s1-spine1</code> key.</p> <pre><code>          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n                - node: s1-spine2 # &lt;- Should not be nested under s1-spine1\n                  ip_address: 10.20.20.3/24\n# &lt;- Newline\n# &lt;- Newline\n</code></pre> <p>We can run pre-commit manually by running the <code>pre-commit run -a</code> command.</p> <pre><code>\u279c  ci-workshops-avd git:(main) \u2717 pre-commit run -a\ntrim trailing whitespace.................................................Passed\nfix end of files.........................................................Failed\n- hook id: end-of-file-fixer\n- exit code: 1\n- files were modified by this hook\n\nFixing sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\n\ncheck yaml...............................................................Failed\n- hook id: check-yaml\n- exit code: 1\n\nwhile parsing a block mapping\n  in \"sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\", line 25, column 17\ndid not find expected key\n  in \"sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml\", line 27, column 17\n\n\u279c  ci-workshops-avd git:(main) \u2717\n</code></pre> <p>We can see the two failures. pre-commit hooks will try and fix errors. However, pre-commit does not assume our intent with the YAML file; that fix is up to us. If you correct the indentation in the file and rerun pre-commit, you will see all passes.</p> <pre><code>          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2 # &lt;- Indentation fixed\n                ip_address: 10.20.20.3/24\n# &lt;- One newline\n</code></pre> <pre><code>\u279c  ci-workshops-avd git:(main) \u2717 pre-commit run -a\ntrim trailing whitespace.................................................Passed\nfix end of files.........................................................Passed\ncheck yaml...............................................................Passed\n\u279c  ci-workshops-avd git:(main)\n</code></pre>","location":"cicd-basics/#pre-commit-example"},{"title":"Filter changes to the Pipeline","text":"<p>Currently, our workflow will build and deploy configurations for both sites. This is true even if we only have changes relevant to one site. We can use a path filter to check if changes within specific directories have been modified, signaling that a new build and deployment are required. Please take note of the <code>id</code> key. This will be referenced in our upcoming workflow steps.</p> <pre><code>...\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n\n      - name: Check paths for sites/site_1\n        uses: dorny/paths-filter@v2\n        id: filter-site1\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_1/**'\n\n      - name: Check paths for sites/site_2\n        uses: dorny/paths-filter@v2\n        id: filter-site2\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_2/**'\n...\n</code></pre>","location":"cicd-basics/#filter-changes-to-the-pipeline"},{"title":"Conditionals to control flow","text":"<p>The Ansible collection install and test configuration steps have the conditional key of <code>if</code>. This maps to each path filter check step we used earlier. For example, the first path check has an <code>id</code> of <code>filter-site1</code>. We can reference the <code>id</code> in our workflow as <code>steps.filter-site1.outputs.workflows</code>. If this is set to <code>true</code>, a change will register in our check, and the test build step for site 1 will run. One difference is the Ansible collection install uses the <code>||</code> (or) operator. The \"or\" operator allows us to control when Ansible collections are installed. The collections will be installed if a change is registered in either <code>filter-site1</code> or <code>filter-site2</code>.</p> <pre><code>...\n      - name: Install collections\n        run: ansible-galaxy collection install -r requirements.yml\n        if: steps.filter-site1.outputs.workflows == 'true' || steps.filter-site2.outputs.workflows == 'true'\n\n      - name: Test configuration for site1\n        run: make build-site-1\n        if: steps.filter-site1.outputs.workflows == 'true'\n\n      - name: Test configuration for site2\n        run: make build-site-2\n        if: steps.filter-site2.outputs.workflows == 'true'\n</code></pre> <p>At this point, make sure both workflow files (<code>dev.yml</code> and <code>prod.yml</code>) within the <code>.github/workflows</code> directory are not commented out. An example of the <code>dev.yml</code> file is below.</p>  .github/workflows/dev.yml <pre><code>name: Test the upcoming changes\n\non:\n  push:\n    branches-ignore:\n      - main\n\njobs:\n  dev:\n    env:\n      LABPASSPHRASE: ${{ secrets.LABPASSPHRASE }}\n    timeout-minutes: 15\n    runs-on: ubuntu-latest\n    steps:\n      - name: Hi\n        run: echo \"Hello World!\"\n\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v3\n\n      - name: Install Python requirements\n        run: pip3 install -r requirements.txt\n\n      - name: Run pre-commit on files\n        uses: pre-commit/action@v3.0.0\n\n      - name: Check paths for sites/site_1\n        uses: dorny/paths-filter@v2\n        id: filter-site1\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_1/**'\n\n      - name: Check paths for sites/site_2\n        uses: dorny/paths-filter@v2\n        id: filter-site2\n        with:\n          filters: |\n            workflows:\n              - 'sites/site_2/**'\n\n      - name: Install collections\n        run: ansible-galaxy collection install -r requirements.yml\n        if: steps.filter-site1.outputs.workflows == 'true' || steps.filter-site2.outputs.workflows == 'true'\n\n      - name: Test configuration for site1\n        run: make build-site-1\n        if: steps.filter-site1.outputs.workflows == 'true'\n\n      - name: Test configuration for site2\n        run: make build-site-2\n        if: steps.filter-site2.outputs.workflows == 'true'\n</code></pre>","location":"cicd-basics/#conditionals-to-control-flow"},{"title":"Day-2 Operations - New service (VLAN)","text":"<p>This example workflow will add two new VLANs to our sites. Site 1 will add VLAN 25, and site 2 will add VLAN 45. An example of the updated group_vars is below. The previous workshop modified the configuration of our devices directly through eAPI. This example will leverage GitHub actions with CloudVision to update our nodes. The provisioning with CVP will also create a new container topology and configlet assignment per device. For starters, we can update site 1.</p>  sites/site_1/group_vars/SITE1_FABRIC_SERVICES.yml <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 10\n            name: 'Ten'\n            tags: [ \"Web\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.10.10.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.10.10.2/24\n              - node: s1-spine2\n                ip_address: 10.10.10.3/24\n          - id: 20\n            name: 'Twenty'\n            tags: [ \"App\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.20.20.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.20.20.2/24\n              - node: s1-spine2\n                ip_address: 10.20.20.3/24\n          - id: 25\n            name: 'Twenty-five'\n            tags: [ \"Wifi\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.25.25.1\n            nodes:\n              - node: s1-spine1\n                ip_address: 10.25.25.2/24\n              - node: s1-spine2\n                ip_address: 10.25.25.3/24\n</code></pre>","location":"cicd-basics/#day-2-operations-new-service-vlan"},{"title":"Build the updates locally (optional)","text":"<p>The pipeline will run the build and deploy steps for us with these relevant changes. We can also run the build steps locally to see all our pending updates.</p> <pre><code>make build-site-1\n</code></pre> <p>Feel free to check out the changes made to your local files. Please make sure the GitHub workflows are uncommented. We can now push all of our changes and submit a pull request.</p>  <p>Note</p> <p>The GitHub workflows are located in the <code>atd-cicd/.github/workflows</code> directory.</p>  <pre><code>git add .\ngit commit -m \"updating VLANs\"\ngit push --set-upstream origin dc-updates\n</code></pre>","location":"cicd-basics/#build-the-updates-locally-optional"},{"title":"Viewing actions","text":"<p>If you navigate back to your GitHub repository, you should see an action executing.</p> <ol> <li>Click <code>Actions</code></li> <li>Click on the latest action</li> </ol> <p></p> <p>Since this is a development branch, we are only testing for valid variable files so that AVD can successfully build our configurations. We can run one more example before deploying to production. You may notice the test configuration step was only initiated for site 1 and was skipped for site 2 (no changes). You can finish this example by updating the site 2 fabric services file.</p>  sites/site_2/group_vars/SITE2_FABRIC_SERVICES.yml <pre><code>---\ntenants:\n  - name: MY_FABRIC\n    vrfs:\n      - name: default\n        svis:\n          - id: 30\n            name: 'Thirty'\n            tags: [ \"DB\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.30.30.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.30.30.2/24\n              - node: s2-spine2\n                ip_address: 10.30.30.3/24\n          - id: 40\n            name: 'Forty'\n            tags: [ \"DMZ\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.40.40.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.40.40.2/24\n              - node: s2-spine2\n                ip_address: 10.40.40.3/24\n          - id: 45\n            name: 'Forty-five'\n            tags: [ \"Guest\" ]\n            enabled: true\n            ip_virtual_router_addresses:\n              - 10.45.45.1\n            nodes:\n              - node: s2-spine1\n                ip_address: 10.45.45.2/24\n              - node: s2-spine2\n                ip_address: 10.45.45.3/24\n</code></pre>  <pre><code>make build-site-2\ngit add .\ngit commit -m \"updating VLANs for site 2\"\ngit push\n</code></pre> <p>Once complete, the GitHub actions will show changes on sites 1 and 2.</p> <p></p>","location":"cicd-basics/#viewing-actions"},{"title":"Creating a pull request to deploy updates (main branch)","text":"<p>We have activated our GitHub workflows and tested our configurations. We are now ready to create a pull request.</p> <p>In your GitHub repository, you should see a tab for Pull requests.</p> <ol> <li>Click on <code>Pull requests</code></li> <li>Click on <code>New pull request</code></li> <li>Change the base repository to be your fork</li> <li>Change the compare repository to <code>dc-updates</code></li> <li>Click <code>Create pull request</code></li> </ol> <p></p> <p></p> <p></p> <p>Add a title and enough of a summary to get the point across to other team members.</p> <p></p> <p>Once this is complete, click <code>Create pull request</code>. Since all checks have passed, we can merge our new pull request. If you have multiple options on the type of merge, select <code>squash and merge</code>.</p> <p></p> <p></p> <p>At this point, this will kick off our second workflow (<code>prod.yml</code>) against the <code>main</code> branch. This workflow will build and deploy our updates with CVP. If you go to the \"Provisioning\" tab of CVP, you should be able to see tasks and pending changes. This workflow will automatically run any pending tasks for us. We can optionally connect to one of the spines at either site to see the new VLANs.</p> <p></p> <pre><code>s1-spine1#show vlan\nVLAN  Name                             Status    Ports\n----- -------------------------------- --------- -------------------------------\n1     default                          active\n10    Ten                              active    Cpu, Po1, Po2\n20    Twenty                           active    Cpu, Po1, Po4\n25    Twenty-five                      active    Cpu, Po1\n4093  LEAF_PEER_L3                     active    Cpu, Po1\n4094  MLAG_PEER                        active    Cpu, Po1\n\ns1-spine1#\n################################################################################\ns2-spine1#show vlan\nVLAN  Name                             Status    Ports\n----- -------------------------------- --------- -------------------------------\n1     default                          active\n30    Thirty                           active    Cpu, Po1, Po2\n40    Forty                            active    Cpu, Po1, Po4\n45    Forty-five                       active    Cpu, Po1\n4093  LEAF_PEER_L3                     active    Cpu, Po1\n4094  MLAG_PEER                        active    Cpu, Po1\n\ns2-spine1#\n</code></pre>","location":"cicd-basics/#creating-a-pull-request-to-deploy-updates-main-branch"},{"title":"Summary","text":"<p>Congratulations, you have successfully deployed a CI/CD pipeline with GitHub Actions. Feel free to make additional site changes or extend the testing pieces.</p>  <p>Note</p> <p>If your topology shuts down or time elapses, you must install the requirements, Git configuration, and GitHub authentication.</p> <p>You must also set the <code>LABPASSPHRASE</code> environment variable in the IDE terminal.</p> <pre><code>export LABPASSPHRASE=`cat /home/coder/.config/code-server/config.yaml| grep \"password:\" | awk '{print $2}'`\n</code></pre>","location":"cicd-basics/#summary"},{"title":"Getting started with Git","text":"<p> </p>","location":"git/"},{"title":"Introduction","text":"<p>In this section, we will explore a brief introduction of Git. We will cover the installation and basic commands used with Git. Git is the most commonly used version control system. Git tracks changes to files allowing you to revert to specific versions. File changes are tracked by storing snapshots (commits) of the files over time. In the image below, the content of files A, B, and C change over time. Git allows you to roll back to any previous commit.</p> <p></p> <p>Git makes collaboration effortless by allowing multiple people to merge their changes into one source. Whether you work solo or as part of a team, Git will be useful for you.</p> <p>Basic Git commands we will be working with:</p> <ul> <li>git config</li> <li>git status</li> <li>git init</li> <li>git add</li> <li>git commit</li> <li>git log</li> <li>git branch</li> <li>git clone</li> <li>git merge</li> <li>git switch</li> <li>git diff</li> <li>git restore</li> </ul>","location":"git/#introduction"},{"title":"Installation &amp; setup","text":"","location":"git/#installation-setup"},{"title":"Installation","text":"Note <p>Git has been pre-installed in your ATD Lab environment. If you need to install and configure Git on another system, follow the instructions at the links above.</p>  <p>Download Git - https://git-scm.com/downloads</p> <p>Configuration - https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</p>","location":"git/#installation"},{"title":"Setup","text":"<p>When setting up Git for the first time, you must configure your Identity with a name and email address. This is used to add your signature to commits. Additionally, set the default branch name to <code>main</code>.</p> <p>Run the following commands from the Terminal in your ATD Lab Programmability IDE.</p> <pre><code># Set your username:\ngit config --global user.name \"FirstName LastName\"\n</code></pre> <pre><code># Set your email address:\ngit config --global user.email \"name@example.com\"\n</code></pre> <pre><code># Set default branch name to `main`\ngit config --global init.defaultbranch main\n</code></pre>","location":"git/#setup"},{"title":"Programmability IDE (VS Code)","text":"<p></p> <p>Verify your configuration:</p> <pre><code>git config --global --list\n</code></pre>","location":"git/#programmability-ide-vs-code"},{"title":"Download Sample Files","text":"<p>We have provided some sample configuration files to begin working with Git. From the Programmability IDE, run the following two commands to download sample files and change your working directory.</p> <pre><code>bash -c \"$(curl https://raw.githubusercontent.com/aristanetworks/ci-workshops-fundamentals/main/get-sample-files.sh)\"\ncd /home/coder/project/labfiles/samplefiles\n</code></pre>","location":"git/#download-sample-files"},{"title":"Git","text":"","location":"git/#git"},{"title":"Git - command line basics","text":"","location":"git/#git-command-line-basics"},{"title":"Initialize the directory as a Git repository","text":"<p>Next, we initialize the current directory <code>/home/coder/project/labfiles/samplefiles/</code> as a repository (repo).</p> <pre><code>git init\n</code></pre> <p>Notice your CLI prompt changed.</p> <p>The directory is now initialized as a Git repository, and the following hidden sub-directory <code>/home/coder/project/labfiles/samplefiles/.git/</code> was created. It holds version control information for your repository.</p> <p> Congratulations!!! You have created your first repository.</p>","location":"git/#initialize-the-directory-as-a-git-repository"},{"title":"Git Repository Status","text":"<p>Check the current status of your repo.</p> <pre><code>git status\n</code></pre> <p>Since this is a brand new repo, you should see output similar to the following, indicating there are untracked files.</p> <pre><code>On branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        leaf1.cfg\n        leaf2.cfg\n        leaf3.cfg\n        leaf4.cfg\n        spine1.cfg\n        spine2.cfg\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n</code></pre>","location":"git/#git-repository-status"},{"title":"Stage your changes","text":"<p>When you want to track files, you first need to stage them. The above output gives you a clue about the command needed to stage the changes. You can specify individual files or add all files with a wildcard period <code>.</code></p> <p>To stage all file changes:</p> <pre><code>git add .\n</code></pre> <p>Then recheck the status to see what is staged and ready to be committed.</p> <pre><code>git status\n</code></pre> <p>Output:</p> <pre><code>On branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   leaf1.cfg\n        new file:   leaf2.cfg\n        new file:   leaf3.cfg\n        new file:   leaf4.cfg\n        new file:   spine1.cfg\n        new file:   spine2.cfg\n</code></pre> <p>All the files are staged and ready to be committed to the <code>main</code> branch.</p>","location":"git/#stage-your-changes"},{"title":"Commit your changes","text":"<p>Now you can commit your staged changes with a comment:</p>  Note <p>Use a comment reflecting the changes made so you can reference this commit in the future. Commit messages will show up in the log.</p>  <pre><code>git commit -m \"Initial Commit\"\n</code></pre> <p>Output:</p> <pre><code>[main (root-commit) 45eeb6d] Initial Commit\n 6 files changed, 832 insertions(+)\n create mode 100644 leaf1.cfg\n create mode 100644 leaf2.cfg\n create mode 100644 leaf3.cfg\n create mode 100644 leaf4.cfg\n create mode 100644 spine1.cfg\n create mode 100644 spine2.cfg\n</code></pre> <p>Now these files are committed to your local repository.</p> <p>Check the status one more time.</p> <pre><code>On branch main\nnothing to commit, working tree clean\n</code></pre> <p>You have successfully stamped history in your repo. Check the log to see what is there.</p> <pre><code>git log\n</code></pre>  Note <p>Press <code>q</code> to quit viewing the log.</p>","location":"git/#commit-your-changes"},{"title":"Create a branch","text":"<p>Creating a branch allows you to make a new copy of your files without affecting the files in the <code>main</code> branch. For example, if you wanted to update the hostnames on your switches, you might create a new branch called <code>update-hostnames</code>.</p> <p></p> <p>Verify the current branch:</p> <pre><code>git branch\n</code></pre> <p>Create a new branch:</p> <pre><code>git branch update-hostnames\n</code></pre> <p>Switch to this new branch:</p> <pre><code>git switch update-hostnames\n</code></pre> <p>Using the IDE, open each switch config file and update the hostname by removing the prefix <code>s1-</code>. Changes are auto-saved.</p> <p>Example: spine1.cfg - change hostname from <code>s1-spine1</code> to <code>spine1</code>.</p> <p>Let's verify the changes (diffs) we are about to commit to ensure they are correct.</p> <pre><code>git diff\n</code></pre> <p>Stage and commit the changes to the new branch <code>update-hostnames</code>.</p> <pre><code>git add .\ngit commit -m \"updated hostname on each switch\"\n</code></pre>","location":"git/#create-a-branch"},{"title":"Merge branch","text":"<p>Now that we are satisfied with our hostname changes, we can merge the <code>update-hostnames</code> branch into <code>main</code>.</p> <p></p> <p>First, switch back to the <code>main</code> branch and notice the hostnames return to the original name. Why did that happen? Remember, we never modified the original copy <code>main</code> branch. This is a different version of the file. Once we merge the <code>update-hostnames</code> branch into <code>main</code>, both copies will be identical.</p> <pre><code>git switch main\n</code></pre> <p>Execute the merge operation:</p> <pre><code>git merge update-hostnames\n</code></pre> <p>Verify the updated hostnames in each file.</p> <p>Now that your changes are merged, you can safely delete the <code>update-hostnames</code> branch.</p> <pre><code>git branch -d update-hostnames\n</code></pre>","location":"git/#merge-branch"},{"title":"GitHub","text":"<p> </p> <p>Before proceeding further, make sure you are logged into your GitHub account.</p> <p>If you do not have a GitHub account, you can create one here.</p>  Note <p>In the ATD Lab, you will authenticate to GitHub using an eight-digit access code. On other systems, you will need a Personal Access Token. You may skip the next step if you work in the Arista-provided ATD Lab IDE. Detailed instructions for creating a Personal Access Token can be found here.</p>","location":"git/#github"},{"title":"Create a GitHub personal access token","text":"<p>To push your local repo to GitHub, you will need a Personal Access Token. From your GitHub account, click through the following path to generate a new personal access token.  Profile \u2192 Settings \u2192 Developer Settings \u2192 Personal Access Tokens \u2192 Tokens (classic) \u2192 Generate new token (classic)</p> <ul> <li>Give the token a meaningful name by setting the Note: <code>MyNewToken</code></li> <li>Set the Expiration: 30 days (default)</li> </ul> <p>Select the scopes to grant to this token. To use your token to access repositories from the command line, select <code>repo</code>. A token with no assigned scopes can only access public information.</p> <p>Click <code>Generate token</code> at the bottom of the page.  Copy and save the token in a secure place. YOU WILL NOT BE ABLE TO SEE THE TOKEN AGAIN.</p> <p></p>","location":"git/#create-a-github-personal-access-token"},{"title":"Fork a repository","text":"<p>A fork is a copy of another repository that you can manage. Forks let you make changes to a project without affecting the original repository. You can fetch updates from or submit changes to the original repository with a pull request.</p> <p>Fork the example Arista CI Fundamentals repository to make your copy.</p>","location":"git/#fork-a-repository"},{"title":"Steps to Fork the example repository","text":"<ol> <li>From GitHub.com, navigate to the Arista CI Fundamentals repository.</li> <li>In the top-right corner of the page, click Fork. </li> <li>Select an owner.</li> <li>Set repository name. By default, forks are named the same as their upstream repository.</li> <li>Optionally, add a description of your fork.</li> <li>Click <code>Create fork</code> button at the bottom</li> </ol> <p>You should now see your repository <code>username/ci-workshops-fundamentals</code> forked from <code>aristanetworks/ci-workshops-fundamentals</code>.</p> <p>Next up... Clone this forked repository to your local host machine.</p>","location":"git/#steps-to-fork-the-example-repository"},{"title":"Clone forked repo to local host","text":"<p>Cloning a repository allows us to make a local copy of a project on GitHub. In the previous step, you forked a repository to your local GitHub account. Navigate to your forked repo in GitHub. From there, click on the green code button and copy the URL of the forked repository.</p> <p></p> <p>Clone this repository to your local machine.</p> <p>Before cloning, change your current directory to <code>/home/coder/project/labfiles/</code>.</p> <pre><code>cd /home/coder/project/labfiles\n</code></pre> <p>Clone the repo.</p> <pre><code># replace this URL with your forked repo\ngit clone https://github.com/xxxxxxx/ci-workshops-fundamentals.git\n</code></pre> <p>Now change into the new cloned directory.</p> <pre><code>cd ci-workshops-fundamentals\n</code></pre> <p>Verify the location of the remote copy. This should be your local GitHub account.</p> <pre><code>git remote -v\n\norigin  https://github.com/xxxxxxx/ci-workshops-fundamentals.git (fetch)\norigin  https://github.com/xxxxxxx/ci-workshops-fundamentals.git (push)\n</code></pre> <p>In the next step, let's add VLAN 40 to the data model in <code>avd/vlans.yml</code>. First, create a new branch called <code>add-vlan-40</code>.</p>","location":"git/#clone-forked-repo-to-local-host"},{"title":"Create and switch to a new branch","text":"<pre><code>git branch add-vlan-40\ngit switch add-vlan-40\n</code></pre> <p>Using the Programmability IDE, update the file <code>ci-workshops-fundamentals/avd/vlans.yml</code> with VLAN 40 information.</p>  Updated vlans.yml <pre><code>---\nvlans:\n  - 10:\n    name: Ten\n  - 20:\n    name: Twenty\n  - 30:\n    name: Thirty\n  - 40:\n    name: Forty\n</code></pre>  <p>Now, stage and commit these changes to the new branch.</p> <pre><code>git add .\ngit commit -m \"added vlan 40\"\n</code></pre>","location":"git/#create-and-switch-to-a-new-branch"},{"title":"Push Changes to GitHub","text":"<p>Now push the updated branch to your remote fork on GitHub.</p> <pre><code># push new branch to remote repo on GitHub\ngit push --set-upstream origin add-vlan-40\n</code></pre>  Note <p>If this is your first push from the Lab environment, you will be prompted to authenticate to GitHub. Follow the <code>Copy &amp; Continue to GitHub</code> prompts by entering the eight-digit authentication code. Additional pushes to GitHub will cache your credentials.</p>  <p>Once authenticated, your new branch and updated file will exist on GitHub.</p> <p>Output:</p> <pre><code>Enumerating objects: 7, done.\nCounting objects: 100% (7/7), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (4/4), 352 bytes | 352.00 KiB/s, done.\nTotal 4 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: Create a pull request for 'add-vlan-40' on GitHub by visiting:\nremote:      https://github.com/xxxxxxxx/ci-workshops-fundamentals/pull/new/add-vlan-40\nremote:\nTo https://github.com/xxxxxxxx/ci-workshops-fundamentals.git\n * [new branch]      add-vlan-40 -&gt; add-vlan-40\nBranch 'add-vlan-40' set up to track remote branch 'add-vlan-40' from 'origin'.\n</code></pre> <p>You should now see the new branch <code>add-vlan-40</code> and commit messages in GitHub.</p> <p>The next step is to merge the <code>add-vlan-40</code> branch into the <code>main</code> branch. A Pull Request is used to do this.</p>","location":"git/#push-changes-to-github"},{"title":"Pull request","text":"<p>A Pull Request in Git allows a contributor (you) to ask a maintainer (owner) of the origin repository to review code changes you wish to merge into a project. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the <code>main</code> branch.</p> <p>Once all changes have been agreed upon, the maintainer of the original repo will merge your changes. At this point, your code changes are visible in the origin project repo.</p>","location":"git/#pull-request"},{"title":"Steps to Initiate a Pull Request","text":"<ol> <li>On GitHub.com, navigate to the main page of your forked repository.</li> <li>In the \"Branch\" menu, choose the branch that contains your commits.</li> <li>Above the list of files, click the Contribute drop-down and click Open pull request. </li> <li>Verify the <code>base repository:</code> is set to <code>aristanetworks/ci-workshops-fundamentals</code> and <code>base:</code> is set to <code>main</code>. Set the <code>head repository:</code> to your forked repo and <code>compare:</code> to the <code>add-vlan-40</code> branch. </li> <li>Add a title and description for your pull request.</li> <li>Click Create pull request.</li> </ol> <p>This will generate a Pull Request on the main project repository <code>aristanetworks/ci-workshops-fundamentals</code>. The owner/maintainer can merge the pull request once all changes are satisfied.</p>  Note <p>During the workshop the PR is not normally merged to prevent having to reset the repo before the next workshop. The <code>Cleanup</code> section below is the normal course of action.</p>","location":"git/#steps-to-initiate-a-pull-request"},{"title":"Cleanup (optional)","text":"<p>After your Pull Request is merged, you may cleanup your old branch and sync your fork.</p> <ol> <li>Delete your branch on GitHub and your local host.</li> <li>Sync your Forked repo (below)</li> <li>Pull the updates into <code>main</code> branch on your local host. <code>git pull</code></li> </ol> <p></p>","location":"git/#cleanup-optional"},{"title":"Welcome to YAML &amp; Jinja","text":"<p></p> <p>This section will cover both Jinja and YAML, which are two interdependent pieces of basic configuration automation framework. While both YAML and Jinja can get relatively complex with what they can accomplish and what can be done with them, we will only cover what is necessary to utilize these tools for network automation and DevOps. At the end of the YAML and JINJA section, we will tie everything together with a final output.</p>","location":"jinja-yaml/"},{"title":"What is YAML?","text":"<p>YAML is what's officially referred to as a data serialization language. While that sounds complex, data serialization is the process of converting objects within a data model into a byte stream for the purpose of storing or transferring it. Breaking this down, we know when trying to automate configuration management or deployment, we need to specify what it is we want to configure, that is, what should our desired end state configuration look like. ( intent based networking )</p> <p>An important note is about why we are talking about and using YAML in the first place. As mentioned in the beginning, YAML is a data serialization language, however, it is not the only one. Some other common, data serialization languages are XML, JSON, and CSV. The reason we are particular towards YAML is that not only are there libraries available in most programming languages, but also because as the following table shows, it is very human readable.</p>   XML JSON YAML    <pre><code>&lt;ntp&gt;\n  &lt;local_interface&gt;\n    &lt;name&gt;Management1&lt;/name&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/local_interface&gt;\n  &lt;servers&gt;\n    &lt;name&gt;time-a-g.nist.gov&lt;/name&gt;\n    &lt;preferred&gt;true&lt;/preferred&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/servers&gt;\n  &lt;servers&gt;\n    &lt;name&gt;utcnist.colorado.edu&lt;/name&gt;\n    &lt;vrf&gt;mgmt&lt;/vrf&gt;\n  &lt;/servers&gt;\n&lt;/ntp&gt;\n</code></pre>   <pre><code>{\n  \"ntp\": {\n    \"local_interface\": {\n      \"name\": \"Management1\",\n      \"vrf\": \"mgmt\"\n    },\n    \"servers\": [\n      {\n        \"name\": \"time-a-g.nist.gov\",\n        \"preferred\": true,\n        \"vrf\": \"mgmt\"\n      },\n      {\n        \"name\": \"utcnist.colorado.edu\",\n        \"vrf\": \"mgmt\"\n      }\n    ]\n  }\n}\n</code></pre>   <pre><code>---\nntp:\n  local_interface:\n    name: Management1\n    vrf: mgmt\n  servers:\n    - name: time-a-g.nist.gov\n      preferred: true\n      vrf: mgmt\n    - name: utcnist.colorado.edu\n      vrf: mgmt\n</code></pre>","location":"jinja-yaml/#what-is-yaml"},{"title":"Common YAML Syntax","text":"<p>Now that we know the idea of what YAML is and what it's used for, we can cover the various constructs within a YAML file, and how to use those expressions to create files that are useful to us as engineers trying to manage configurations.</p>","location":"jinja-yaml/#common-yaml-syntax"},{"title":"Comments","text":"<p>Comments are an important part of any documentation or configuration you are doing. When creating or modify any network configurations, while you don't realize it, you are commenting something that is important. When configuring an interface, you may add a description, such as the name of a downstream device or server it's connected to. Or if its a provider handoff you may add the NOC telephone number and the circuitID. If you are configuring BGP peers, you may add a description to each peer so it's clear what the neighbor is.</p> <p>In YAML, you can enter comments anywhere you'd like, and ideally, where it makes sense to help explain what your data model is representing. Comments in YAML are represented with the pound symbol, and shown below.</p> <pre><code>---\n# This is my YAML vars file for global configuration settings.\nsome config:  here\n\n# Management Interface Configuration\nmgmt_gateway: 192.168.0.1\nmgmt_interface: Management0\nmgmt_interface_vrf: default\n</code></pre>  Tip <p> In VS Code, you can auto-comment any text you want by selecting the text and pressing windows Ctrl + / or mac Cmd + /.</p>","location":"jinja-yaml/#comments"},{"title":"Mappings","text":"<p>The first, and basic YAML construct is a mapping, which comprises of what is technically called a <code>scalar</code>. This will probably be the only time you hear or reference the word scalar, ever.</p> <pre><code>---\n# These are some examples of scalars\n\ninteger: 10\nboolean: true\nstring: \"Welcome to the Automation Workshop\"\n</code></pre> <p>Sometimes, a scalar is referred to as only the initial part of the line of text such as a list of names or locations like such.</p> <pre><code>---\n# Another scalar example\n\n- White Rim Trail\n- Hells Revenge\n- Long Canyon Road\n- Black Bears Pass\n- Ophir Pass\n</code></pre> <p>In normal day to day use, you will see scalars referred to as key value pair mappings, or just mappings for short. A mapping is the data label, followed by its value, separated by a <code>:</code> colon.</p> <p>Here are some network config specific mappings:</p> <pre><code>---\n# Network Config Mappings\n\nmgmt_interface: Management1\nmgmt_interface_vrf: MGMT\n\nspanning_tree_mode: rapid-pvst\nspanning_tree_priority: 4096\n</code></pre>","location":"jinja-yaml/#mappings"},{"title":"Boolean","text":"<p>Another type of mapping in YAML are booleans. Booleans are simply a <code>true</code> or <code>false</code> value assigned to its data label. While the actual true or false does not have to be in a certain case, if you want the value to be compatible with lint options, you should use all lowercase.</p> <p>Here are some network config boolean mappings.</p> <pre><code>---\n# Network Config Mappings\n\nevpn_import_pruning: true\n\nevpn_gateway:\n        evpn_l2:\n          enabled: true\n\nmlag: false\n</code></pre>","location":"jinja-yaml/#boolean"},{"title":"Lists","text":"<p>Lists, which are sometimes also called sequences, are similar to arrays in development. A list has a top level label, or what I like to refer to as a key, which represent the elements in the list. ( The key nomenclature will come in handy when thinking through loops in Jinja templates )  In a standard list, you will have singular entries below the top level label.</p> <p>Here are some examples of lists:</p> <pre><code>---\n# Some lists of BGP specific configurations\n\nbgp-peers:\n    - 192.168.103.1\n    - 192.168.103.3\n    - 192.168.103.5\n\nbgp_defaults:\n    - 'no bgp default ipv4-unicast'\n    - 'distance bgp 20 200 200'\n    - 'neighbor default send-community'\n    - 'graceful-restart restart-time 300'\n    - 'graceful-restart'\n</code></pre>","location":"jinja-yaml/#lists"},{"title":"Dictionary","text":"<p>While we just covered lists, which allow you to specify an item that falls inline below a parent label or key, what happens when you want to include more specific attributes to that parent label, or if you want to introduce more details such as key-value pair mappings to your list? In that occasion, what is used, and what you will see regularly in just about every network vars file, is a dictionary. As previously described, a dictionary is a list of key-value mappings.</p> <p>Here is an example of a dictionary:</p> <pre><code>---\nl3spine:\n  defaults:\n    platform: 720XP\n    spanning_tree_mode: rapid-pvst\n    spanning_tree_priority: 4096\n    loopback_ipv4_pool: 10.0.0.0/24\n    mlag_peer_ipv4_pool: 192.0.0.0/24\n    mlag_peer_l3_vlan: false\n    virtual_router_mac_address: aa:aa:bb:bb:cc:cc\n    mlag_interfaces: [Ethernet53, Ethernet54]\n    mlag_port_channel_id: 2000\n</code></pre>","location":"jinja-yaml/#dictionary"},{"title":"Nested Data Structures","text":"<p>One of the double edged parts of YAML is that it can be quite complex. This is good because it is very flexible for how we can build our data model, but it also means that model can get out of control. YAML syntax is hierarchical, and at the same time, just about every construct we covered previous to this can be nested within each other. Lets take a look at some nested data structures that illustrate this.</p> <p>In this first example we will have a list of dictionaries:</p>  Reminder <p> Indentation is key.</p>  <pre><code>---\n# local users\n\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  cvpadmin:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  ansible_user:\n    privilege: 15\n    role: network-admin\n    sha512_password: encrypted_pass\n  noc:\n    privilege: 1\n    role: network-operator\n    sha512_password: encrypted_pass\n</code></pre> <p>The following example will show another network device specific nested data structure which mixes dictionaries and lists in different places:</p> <pre><code>---\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      preferred: true\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n\nsvis:\n  10:\n    name: 'DATA'\n    tags: ['Web']\n    enabled: true\n    ip_virtual_router_addresses:\n      - 10.10.10.1\n    mtu: 9214\n    ip_helpers:\n      10.100.100.20:\n        source_interface: Management1\n        source_vrf: MGMT\n      127.0.0.1:\n    nodes:\n      SW-CORE-A:\n        ip_address: 10.10.10.2/23\n      SW-CORE-B:\n        ip_address: 10.10.10.3/23\n</code></pre>","location":"jinja-yaml/#nested-data-structures"},{"title":"YAML File Examples","text":"<p>Now that we have gone through all the most common constructs within a YAML file used for variables for network device configs, lets see what some complete files would look like for a real world environment.</p>","location":"jinja-yaml/#yaml-file-examples"},{"title":"Network Config Example","text":"<p>These example YAML files could be used to build a base config for a series of devices. The devices would be based on your inventory file, however, for this example, we will assume there is a layer2 leaf/spine topology, with two spines and two leafs. For this example we will use two files, a <code>global.yml</code> and a <code>interface.yml</code> file.</p> <p>The global.yml file that follows includes the data model used for the base configuration. These items apply to all four devices in the fabric. ( This could be imported in the playbook, or put in the <code>group_vars</code> directory and named after the level of your hierarchy that contains the devices you want this to apply to. )</p> <p><code>global.yml</code></p> <pre><code>---\n# local users\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    secret: aristaadmin\n  noc:\n    privilege: 1\n    role: network-operator\n    secret: aristaops\n\n# aaa authentication and authorization\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\n# radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: 055A0A5D311E1E5B4944\n\n# HTTP Client source interface and VRF\nip_http_client_source_interfaces:\n    - name: Management1\n      vrf: MGMT\n\n# RADIUS source interface and VRF\nip_radius_source_interfaces:\n  - name: Management1\n    vrf: MGMT\n\n#MAC and ARP aging timers\nmac_address_table:\n  aging_time: 1800\n\narp:\n  aging:\n    timeout_default: 1500\n\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n\n# DNS lookup source interface (Servers defined in 1L2P.yml)\nip_domain_lookup:\n  source_interfaces:\n    Management1:\n      vrf: MGMT\n\n# NTP Servers (source interface defined in group specific YML files (CORE, ACCESS, MGMT, INET)\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      preferred: true\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre> <p><code>interface.yml</code></p> <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\nleaf1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\nleaf2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\n</code></pre>","location":"jinja-yaml/#network-config-example"},{"title":"Jinja","text":"","location":"jinja-yaml/#jinja"},{"title":"What is Jinja?","text":"<p>At a high level, Jinja is a templating engine used to create markup files such as HTML or XML as well as custom text files, such as in our instance config files. Under the hood, Jinja is an open source python library that lets you create extensible templates. One of the major benefits of Jinja is that the template files you create allow you to define both static text, as well as variables. Some of the Jinja template syntax may look familiar because Jinja is not the first templating engine, and is actually inspired by Django.</p>","location":"jinja-yaml/#what-is-jinja"},{"title":"What is Jinja Used For?","text":"<p>As it may have become apparent from the YAML section, after creating our data model of the various configuration parameters we want to automate, we need to get that data model and all its variables into a format that can read and understood by our network devices. This is where Jinja comes in. The use of Jinja templates, along with some yet to be shown Ansible magic, allows us to render full or partial configuration files that can loaded onto network devices. The underlying purpose of this, as it relates to automation, is that with the use of various expressions and variables in the Jinja templates, we can use a single template, with single or multiple YAML variable files, and create configurations against a multitude of network devices. As far as the actual template file and its file extension, technically any file can be called as a template regardless of its extension as long as its formatted correctly, however, we typically use the <code>.j2</code> extension on all Jinja template files.</p>","location":"jinja-yaml/#what-is-jinja-used-for"},{"title":"Jinja Syntax","text":"<p>As a foreword to getting into the different tasks we can accomplish in our Jinja templates, it's important to call out that there are a few common expressions that are used throughout all Jinja templates, including those related to network devices. They are outlined below.</p> <p><code>Comments:</code></p> <p>Comments are represented as such, with our friend the pound symbol:</p> <pre><code>{# Automation Is Fun #}\n</code></pre> <p><code>Expressions/Variables</code></p> <p>Expressions or variables are represented with a pair of curly brackets:</p> <pre><code> {{ inventory_hostname }}\n</code></pre> <p><code>Statements</code></p> <p>Statements are represented with a percent symbol:</p> <pre><code>{% for items in vars_file['interfaces'] %}\n{% endfor %}\n</code></pre>  Live Jinja Parser <p>Quick and easy method for testing data models and Jinja syntax: https://j2live.ttl255.com/</p>","location":"jinja-yaml/#jinja-syntax"},{"title":"Inventory File and Ansible Playbook","text":"<p>While we won't cover the inventory file or Ansible playbooks in depth in this section as it will be covered in Ansible, it is important to call out it's importance in relation to Jinja. You may wonder, when using Ansible to automate and render any configurations, how does it know what devices I want to create configurations for? This is accomplished with the Ansible inventory file. This will be covered more thoroughly, however, for the purpose of the following examples, we will assume we have an inventory file with four hosts, <code>spine1</code>, <code>spine2</code>, <code>leaf1</code>, and <code>leaf2</code>. Some examples may show output for all four devices, and some may show output for just one, depending on the complexity of the example. Also, every example we show will use the following Ansible Playbook so you understand the destination filename syntax. There are more parts to this playbook but they will be shown at the end and covered in the next presentation:</p> <pre><code>---\n# Inventory File\nfabric:\n  spines:\n    spine1:\n    spine2:\n  leafs:\n    leaf1:\n    leaf2:\n</code></pre> <pre><code>---\n#Config Playbook\n- hosts: spine1,spine2,leaf1,leaf2\n  gather_facts: false\n  - name: Register variables\n    include_vars:\n      file: \"{{lookup('env','PWD')}}/vars/global.yml\"\n      name: global\n  - name: Register variables\n    include_vars:\n      file: \"{{lookup('env','PWD')}}/vars/interface.yml\"\n      name: interface\n  - name: Create configs\n    template:\n      src: \"{{lookup('env','PWD')}}/templates/full_config.j2\"\n      dest: \"{{lookup('env','PWD')}}/configs/full_config/{{inventory_hostname}}_config.cfg\"\n</code></pre>","location":"jinja-yaml/#inventory-file-and-ansible-playbook"},{"title":"Variable Substitution","text":"<p>As shown previously, we know expressions or variable substitution is performed with the double curly brackets, <code>{{ my_var }}</code>, but what does this look like in a Jinja template?</p> <p>For example, we may want to generate the hostname in our template for all the devices in our inventory file. In order to this we can use a standard Ansible variable called <code>inventory_hostname</code>, which substitutes in the current name of the inventory host the Ansible play is running against.</p> <pre><code>{# Create a file assigning the device hostname #}\n\nhostname {{ inventory_hostname }}\n</code></pre> <p>Assuming our pre-defined inventory file, running Ansible against this template with the relevant YAML file called would yield four different output files:</p> <pre><code>spine1_config.cfg\nspine2_config.cfg\nleaf1_config.cfg\nleaf2_config.cfg\n</code></pre> <p>The output of one of these files all listed below would be as follows:</p> <pre><code>#spine1_config.cfg\nhostname spine1\n\n#spine2_config.cfg\nhostname spine2\n\n#leaf1_config.cfg\nhostname leaf1\n\n#leaf2_config.cfg\nhostname leaf2\n</code></pre> <p>How about a more complex variable substitution using something from one of our data models above. This shows how to substitute for a single dictionary item:</p> <pre><code># Global.yml\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre> <p>The Jinja template to call these variables is shown here:</p> <pre><code># Render aaa authC config line\naaa authentication login default {{ global['aaa_authentication']['login']['default'] }}\n\n# Render aaa authZ config line\naaa authorization exec default {{ global['aaa_authorization']['exec']['default'] }}\n\n# Render clock timezone config line\nclock timezone {{ global['clock']['timezone']}}\n</code></pre> <p>As you can see, the variable has many parameters in them. Lets walk through these parameters using the <code>aaa authentication</code> config line.</p> <p><code>global</code>:  Global is the name of the vars file when we registered it in our Ansible playbook. You can register this as anything you want.</p> <p><code>aaa_authentication</code>:  This is the name of the parent or top level label or key in our dictionary. This tells the Jinja template the next level to look for the variable we substitute.</p> <p><code>login</code>: This is another label or key down the dictionary, and again tell us where to keep looking for the final variable.</p> <p><code>default</code>:  This is the key-value pair mapping in the dictionary that we want assigned as the variable. We see this is the final parameter in our variable substitution because we want the value of that key to be the result.</p> <p>Running the playbook generates the following configuration against all devices called in the inventory file:</p>  Configuration Output <pre><code># AAA authC Login\naaa authentication login default group radius local\n\n# AAA authZ Login\naaa authorization exec default group radius local\n\n# Device Timezone\nclock timezone America/Detroit\n</code></pre>","location":"jinja-yaml/#variable-substitution"},{"title":"Conditionals and Loops","text":"","location":"jinja-yaml/#conditionals-and-loops"},{"title":"Conditionals","text":"<p>Conditionals, such as <code>{% if %}</code>, <code>{% elif %}</code>, and <code>{% else %}</code>, as well as <code>{% for %}</code> loops are extremely helpful for either or configurations that may apply to only a subset of devices you are generating configurations for. Additionally, for loops are a must for efficiently working through nested data structures like lists of lists or lists of dictionaries.</p> <p>Lets start our statements journey with conditionals, and where they can be helpful.</p> <p>In this first example we will look at the following portion of our <code>interfaces.yml</code> YAML vars file:</p> <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\n</code></pre> <p>The following dictionary key is what we are going to pay close attention to for our Jinja template: <code>mlag_side</code></p> <p>Here we will use a Jinja template to create a partial, correct MLAG configuration per spine device, based on which side it is.</p> <pre><code>{% if interface[inventory_hostname]['mlag_side'] == 'A' %}\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n\n{% else %}\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n\n{% endif %}\n</code></pre> <p>Reviewing this template, we can see we are using a conditional based on which MLAG side the device is. If the device belongs to side A, it will apply the matching configuration. If the device belongs to side B it will apply the other configuration.</p> <p>Here is the output of running the Ansible playbook and the configs that are generated:</p>  spine1 Output <pre><code>int vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre>   spine2 Output <pre><code>int vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre>  <p>Another aspect of conditionals to match on is a boolean, whether something is true or false, using the following example, also referencing our <code>interfaces.yml</code>:</p> <pre><code>{% for item in interface[inventory_hostname]['interfaces'] %}\n{% if interface[inventory_hostname]['interfaces'][item]['mlag_peerlink'] == true %}\ninterface {{ item }}\ndescription {{ interface[inventory_hostname]['interfaces'][item]['desc'] }}\nchannel-group 2000 mode active\n{% else %}\n\ninterface {{ item }}\ndescription {{ interface[inventory_hostname]['interfaces'][item]['desc'] }}\n\n{% endif %}\n{% endfor %}\n</code></pre> <p>Ignoring for a moment the for loop part we haven't covered yet, we can see we are checking each interface that is defined to see if the interface parameter for <code>mlag_peerlink</code> is set to true, if it is, we want to apply an extra line of configuration, <code>channel-group 2000 mode active</code>, if it's not, we just configure the specified interface with a description. The output of running this against the spines would be as follows:</p>  spine1 Output <pre><code>interface Ethernet47\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet1\ndescription TO_LEAF1\n\ninterface Ethernet2\ndescription TO_LEAF2\n</code></pre>   spine2 Output <pre><code>interface Ethernet47\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet1\ndescription TO_LEAF1\n\ninterface Ethernet2\ndescription TO_LEAF2\n</code></pre>","location":"jinja-yaml/#conditionals"},{"title":"Loops","text":"<p>Another great function that Jinja templates support is the use of <code>for</code> loops. For loops come in handy when trying to iterate through a list of dictionaries to repeat configuration lines.</p> <p>Let's start with an example using a <code>for</code> loop to iterate through a list. For example, the below list shows a list with a single DNS server:</p> <pre><code># DNS Servers\nname_servers:\n  - 10.100.100.20\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for dns in global['name_servers'] %}\nip name-server {{ dns }}\n{% endfor %}\n</code></pre> <p>Lets analyze the sections of this template.</p> <p><code>dns</code>:  DNS is a variable that we are using to represent each item in the list. This can be anything you wish.</p> <p><code>global</code>:  If you recall, this is the name of the YAML file we are registering in our playbook. This tells the template which YAML file to look at for the variable.</p> <p><code>name_servers</code>:  This is the parent label or key of the list. This says which items in the list we want to iterate through and assign to the variable we defined.</p> <p>After assigning the value in the list to our created variable, we issue our configuration line which contains static text and our variable. The output would look as follows:</p> <pre><code>ip name-server 10.100.100.20\n</code></pre> <p>What if we had multiple items in the list like this:</p> <pre><code># DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n</code></pre> <p>The for loop would run as many times as there are items in the list and the configuration file output would look as follows:</p> <pre><code>ip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n</code></pre> <p>Now lets take a look at a slightly more complex, nested data structure, such as a dictionary with a list item. We will use the following portion from our data model:</p> <pre><code># radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: radiusserverkey\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for rsrv in global['radius_servers'] %}\nradius-server host {{ rsrv['host'] }} vrf {{ rsrv['vrf'] }} key {{ rsrv['key'] }}\n{% endfor %}\n</code></pre> <p>Lets analyze the sections of this template.</p> <p><code>rsrv</code>:  This is a variable we are setting that represents each item in the list of the dictionary <code>radius_servers</code>.</p> <p><code>global</code>: This tells the template which YAML file to look at for the variable.</p> <p>Looking at the configuration line we are creating, we can see instead of walking through the dictionary via the dictionary key names, we are keying off the our variable which represents the items in our dictionary list. This can be seen with the <code>rsrv['hosts']</code> line. This means we are looking for the value of the <code>host</code> key for each server in our list that is currently assigned to the <code>rsrv</code> variable. The same holds true for the <code>rsrv['vrf']</code> and <code>rsrv['key']</code> lines.</p> <p>The for loop would run as many times as there are items in the list, which is just one, and the configuration file output would look as follows:</p> <pre><code>radius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n</code></pre> <p>While that was simple, what if we have something more complex, like a dictionary with a list of dictionaries? Lets take a look at how this would be represented in Jinja using the following YAML data model from our <code>global.yml</code> file.</p> <pre><code>ntp:\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n</code></pre> <p>The template with a for loop to iterate through this list would be as follows:</p> <pre><code>{% for ntps in global['ntp']['servers'] %}\nntp server vrf {{ ntps['vrf'] }} {{ ntps['name'] }}\n{% endfor %}\n</code></pre> <p>Lets analyze the sections of this template.</p> <p><code>ntps</code>:  This is another variable we are setting that represents each item in the list of the dictionary servers.</p> <p><code>global</code>: This tells the template which YAML file to look at for the variable.</p> <p>Looking at the configuration line we are creating, we can see instead of walking through the dictionary via the dictionary key names, we are keying off the our variable which represents the items in our dictionary list. This can be seen with the <code>ntps['vrf']</code> line. This means we are looking for the value of the <code>vrf</code> key for each server in our list that is currently assigned to the <code>ntps</code> variable. The same holds true for the <code>ntps['name']</code> line.</p> <p>The for loop would run as many times as there are items in the list and the configuration file output would look as follows:</p> <pre><code>ntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n</code></pre> <p>In the previous examples we only covered single for loops iterating a single layer deep, however, what if we have a dictionary within a dictionary, which contains a list? In that instance we would need to have nested for loops in our templates. For this example we will introduce a new YAML data model to better illustrate nested for loops. This section shows some configuration you may need to apply related to EVPN VXLAN:</p> <pre><code>---\n# EVPN VXLAN vrfs\nRed:\n  l3vni: 10000\n  vlans:\n    101:\n      l2vni: 10001\n      anycast_gw: 10.10.10.1/24\n    102:\n      l2vni: 10002\n      anycast_gw: 10.10.20.1/24\nBlue:\n  l3vni: 20000\n  vlans:\n    201:\n      l2vni: 20001\n      anycast_gw: 10.20.10.1/24\n    202:\n      l2vni: 20002\n      anycast_gw: 10.20.20.1/24\n</code></pre> <p>Reviewing this data model we can see we have two dictionaries that represent different VRFs, <code>Red</code> and <code>Blue</code>. Within each of those dictionaries we have another dictionary called <code>vlans</code>, which defines the interfaces in each of those VRFs. When performing the configuration of say the VXLAN interface where we would need to define both the l3vni, as well as each VLAN to VXLAN mapping, we would need to loop through each of the parent keys, then each of the items in <code>vlans</code> dictionary.</p> <p>The template to accomplish this would look like this:</p> <pre><code>{# Leaf evpn config #}\ninterface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\n\n{% for vrf,vrf_values in vrf.items() %}\n\nvxlan vrf {{vrf}} vni {{ vrf_values['l3vni']}}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n\nvxlan vlan {{ vlan }} vni {{ vlan_values['l2vni'] }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Looking at this template, lets walk through whats happening. First, we have a for loop that is iterating through the top level dictionary keys in the YAMl file itself. Those keys would be <code>Red</code> and <code>Blue</code>. Looking at the actual for loop syntax itself, we have some new parameters to look at:</p> <p><code>vrf</code>:  This is a variable the keys in the dictionary list are assigned to.</p> <p><code>vrf_vales</code>:  This is another variable the corresponding values of those keys are assigned to.</p> <p><code>vrf.items()</code>: This uses the built in .items function to determine which items in the dictionary we are looping through. It includes the top level keys.</p> <p>Using those parameters, we construct our configuration line, which includes the variable <code>vrf</code>, the key, and the value for the pair <code>l3vni</code>.</p> <p>After this we repeat the same loop logic, but for the nested dictionary. In this instance we create a new set of variables for the keys and values, only this time, those are assigned the values within the <code>vlans</code> dictionary, as called by the <code>vrf_values['vlans'].items()</code> parameter.</p> <p>Before we view the final output, lets look at what the variable output actually is. This will both make it clearer what they represent based on the data model, as well as show you a nice way to troubleshoot your template while you are working on it.</p> <p>First we will look at the <code>vrf</code> variables, and we will do this by changing the template to this:</p> <pre><code>{# Leaf evpn config #}\n{# interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789 #}\n{% for vrf,vrf_values in vrf.items() %}\n{# vxlan vrf {{vrf}} vni {{ vrf_values['l3vni']}}  #}\n{{ vrf }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n{# vxlan vlan {{ vlan }} vni {{ vlan_params['l2vni'] }} #}\n{{ vlan }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Reviewing the above template, we will comment out everything except for our for loop and out initial key variables. The result of this template will run through the nested loops, and output the value of the variables called, <code>{{ vrf }}</code>, and <code>{{ vlan }}</code>.</p> <p>Viewing the output, we can see that these equal the top level keys of our data model:</p> <pre><code>Red\n\n101\n\n102\n\n\nBlue\n\n201\n\n202\n</code></pre> <p>The first for loop runs and you can see <code>vrf=Red</code>. Then the nested loop runs twice due to there being two VLANs, and we can see both VLAN values. This then repeats for the second top level key, <code>Blue</code>.</p> <p>Now lets take a look at the <code>_params</code> variable. Changing the template to what follows and commenting out our initial variable, and adding a substitution line for the <code>_params</code> variable:</p> <pre><code>{# Leaf evpn config #}\n{# interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789 #}\n{% for vrf,vrf_values in vrf.items() %}\n{# vxlan vrf {{vrf}} vni {{ vrf_values['l3vni']}}  #}\n{# {{ vrf }} #}\n{{ vrf_values }}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n{# vxlan vlan {{ vlan }} vni {{ vlan_params['l2vni'] }} #}\n{# {{ vlan }} #}\n{{ vlan_values }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <p>Viewing the output, we can see that these equal the dictionary key-pair values of the nested <code>vlan</code> dictionary:</p> <pre><code>{'l3vni': 10000, 'vlans': {101: {'l2vni': 10001, 'anycast_gw': '10.10.10.1/24'}, 102: {'l2vni': 10002, 'anycast_gw': '10.10.20.1/24'}}}\n\n{'l2vni': 10001, 'anycast_gw': '10.10.10.1/24'}\n\n{'l2vni': 10002, 'anycast_gw': '10.10.20.1/24'}\n\n\n{'l3vni': 20000, 'vlans': {201: {'l2vni': 20001, 'anycast_gw': '10.20.10.1/24'}, 202: {'l2vni': 20002, 'anycast_gw': '10.20.20.1/24'}}}\n\n{'l2vni': 20001, 'anycast_gw': '10.20.10.1/24'}\n\n{'l2vni': 20002, 'anycast_gw': '10.20.20.1/24'}\n</code></pre> <p>Reverting back to our desired template, we can see what the template and actual output would be:</p> <pre><code>{# Leaf evpn config #}\ninterface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\n\n{% for vrf,vrf_values in vrf.items() %}\n\nvxlan vrf {{vrf}} vni {{ vrf_values['l3vni']}}\n\n{% for vlan,vlan_values in vrf_values['vlans'].items() %}\n\nvxlan vlan {{ vlan }} vni {{ vlan_values['l2vni'] }}\n\n{% endfor %}\n{% endfor %}\n</code></pre> <pre><code>interface vxlan1\nvxlan source-interface loopback1\nvxlan udp-port 4789\nvxlan vrf Red vni 10000\nvxlan vlan 101 vni 10001\nvxlan vlan 102 vni 10002\nvxlan vrf Blue vni 20000\nvxlan vlan 201 vni 20001\nvxlan vlan 202 vni 20002\n</code></pre>","location":"jinja-yaml/#loops"},{"title":"Filters","text":"<p>The final section we will cover will be filters. While there are an enormous amount of filters available, we will just cover a very common one, the <code>ipaddr</code> filter. In a simple explanation, the <code>ipaddr</code> filter takes a full IP address and subnet mask, and strips off just the mask, with the end result being only the address. This can be helpful in an instance where you are using a full prefix and mask in your data model and don't want to create a new, duplicate, key-value pair mapping to be called.</p>","location":"jinja-yaml/#filters"},{"title":"IPADDR()","text":"<p>In a simple explanation, the <code>ipaddr</code> filter has various operations allowing you to manipulate a prefix or network and obtain certain information about it. This can be helpful in an instance where you are using a full prefix and mask in your data model and don't want to create a new, duplicate, key-value pair mapping to be called.</p> <p>The following are some of the functions available within the ipaddr() filter:</p> <p><code>address:</code> When using a prefix in x.x.x.x/yy notation, this filter will pull only the address portion.</p> <p><code>network:</code> This filter will calculate the network ID of a given prefix.</p> <p><code>netmask:</code> This filter will expand out the subnet mask from /yy notation.</p> <p><code>broadcast:</code> This filter will calculate the broadcast address of a given prefix.</p> <p><code>size:</code> This filter will calculate the size of a subnet based on the subnet mask.</p> <p><code>range_usable:</code> This filter finds the range of usable addresses within a subnet.</p> <p>To illustrate this, we will use a simple example for showing the IP info of these two interfaces run against a single device, spine1.</p> <p>The data model is as follows:</p> <pre><code>---\nspine1:\n  interfaces:\n    Ethernet1:\n      ipv4: 1.1.1.1/26\n    Ethernet2:\n      ipv4: 2.2.2.2/8\n</code></pre> <p>The Jinja template is as follows, which will use the above described filters to give us various information about the IP addresses assigned to Ethernet1 and Ethernet2:</p> <pre><code>Here is some information about some of Spine1's interfaces:\n\nEthernet1\n    IP: {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('address')  }}\n    Subnet Mask:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('netmask')  }}\n    Network ID:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('network')  }}\n    Network Size:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('size')  }}\n    Usable Range:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('range_usable')  }}\n    Broadcast Address:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet1']['ipv4'] | ipaddr('broadcast')  }}\n\nEthernet2\n    IP: {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('address')  }}\n    Subnet Mask:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('netmask')  }}\n    Network ID:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('network')  }}\n    Network Size:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('size')  }}\n    Usable Range:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('range_usable')  }}\n    Broadcast Address:  {{ ip_filter[inventory_hostname]['interfaces']['Ethernet2']['ipv4'] | ipaddr('broadcast')  }}\n</code></pre> <p>Reviewing our template, we can see we are using the current inventory device we are running the playbook against, and keying in on the ipv4 address mapping. We are then using the filter command, <code>|</code>, and specifying the <code>address</code> keyword, meaning we only want the address part of the whole prefix.</p> <p>Running the playbook results in the following output:</p> <pre><code>Here is some information about some of Spine1's interfaces:\n\nEthernet1\n    IP: 1.1.1.1\n    Subnet Mask:  255.255.255.192\n    Network ID:  1.1.1.0\n    Network Size:  64\n    Usable Range:  1.1.1.1-1.1.1.62\n    Broadcast Address:  1.1.1.63\n\nEthernet2\n    IP: 2.2.2.2\n    Subnet Mask:  255.0.0.0\n    Network ID:  2.0.0.0\n    Network Size:  16777216\n    Usable Range:  2.0.0.1-2.255.255.254\n    Broadcast Address:  2.255.255.255\n</code></pre>","location":"jinja-yaml/#ipaddr"},{"title":"JOIN()","text":"<p>When working with lists in YAML, it may be needed to have all items in a list concatenated into a single line of a configuration command, instead of looping through the list and creating a configuration command for each item. Some examples of this could be configuring a list of NTP or DNS servers in a single line, versus individual entries. This can be accomplished with the following filter:</p> <p><code>join(\" \"):</code> This filter joins all the items in the list. Pay close attention to the space between the double quotes. This provides spacing between the items in a list. Without this space, all items in the list would be joined into one continuous string.</p> <p>To illustrate this, we will use simple DNS server data model and Jinja template.</p> <p>The data model is as follows:</p> <pre><code>---\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n</code></pre> <p>The following Jinja template will allow us to configure all four DNS servers on a single line for a single configuration command:</p> <pre><code>ip name-server {{ global['name_servers'] | join(\" \") }}\n</code></pre> <p>Running the playbook results in the following output:</p> <pre><code>ip name-server 10.100.100.20 8.8.8.8 4.4.4.4 208.67.222.222\n</code></pre>","location":"jinja-yaml/#join"},{"title":"The Jinja YAML Relationship","text":"<p>As has hopefully been explained well, the interdependent relationship between Jinja and YAML is that Jinja templates utilize the YAML vars files and their data model to generate new configurations, whether full or partial. YAML files serve no purpose without being called and their variables used, and configurations rendered from Jinja templates would be no different than static configs without the variable substitution of YAML files.</p>","location":"jinja-yaml/#the-jinja-yaml-relationship"},{"title":"Jinja Templates","text":"","location":"jinja-yaml/#jinja-templates"},{"title":"Network Config Template Example","text":"<p>Using all the above sections we have reviewed, lets put it all together into a template called <code>full_config.j2</code>, which renders an entire network config based on the <code>global.yml</code> and <code>interface.yml</code> YAML files.</p> <p>The Jinja template would look as follows:</p>  full_config.j2 <pre><code>{# Full Config #}\n\nhostname {{ inventory_hostname }}\n\nusername admin priv {{ global['local_users']['admin']['privilege'] }} role {{ global['local_users']['admin']['role'] }} secret {{ global['local_users']['admin']['secret'] }}\n\nusername noc priv {{ global['local_users']['noc']['privilege'] }} role {{ global['local_users']['noc']['role'] }} secret {{ global['local_users']['noc']['secret'] }}\n\naaa authentication login default {{ global['aaa_authentication']['login']['default'] }}\n\naaa authorization exec default {{ global['aaa_authorization']['exec']['default'] }}\n\n{% for rsrv in global['radius_servers'] %}\nradius-server host {{ rsrv['host'] }} vrf {{ rsrv['vrf'] }} key {{ rsrv['key'] }}\n{% endfor %}\n\n{% for httpsrc in global['ip_http_client_source_interfaces'] %}\nip http client local-interface {{ httpsrc['name'] }} vrf {{ httpsrc['vrf'] }}\n{% endfor %}\n\n{% for radsrc in global['ip_radius_source_interfaces'] %}\nip radius vrf {{ radsrc['vrf'] }} source-interface {{ radsrc['name'] }}\n{% endfor %}\n\nmac address-table aging-time {{ global['mac_address_table']['aging_time']}}\n\narp aging timeout {{ global['arp']['aging']['timeout_default']}}\n\n{% for dns in global['name_servers'] %}\nip name-server {{ dns }}\n{% endfor %}\n\nip domain lookup vrf {{ global['ip_domain_lookup']['source_interfaces']['Management1']['vrf'] }} source-interface Management1\n\n\n{% for ntps in global['ntp']['servers'] %}\nntp server vrf {{ ntps['vrf'] }} {{ ntps['name'] }}\n{% endfor %}\n\nclock timezone {{ global['clock']['timezone']}}\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\n{% for item in interface[inventory_hostname]['interfaces'] %}\n{% if interface[inventory_hostname]['interfaces'][item]['mlag_peerlink'] == true %}\ninterface {{ item }}\ndescription {{ interface[inventory_hostname]['interfaces'][item]['desc'] }}\nchannel-group 2000 mode active\n{% else %}\n\ninterface {{ item }}\ndescription {{ interface[inventory_hostname]['interfaces'][item]['desc'] }}\n\n{% endif %}\n\n{% endfor %}\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\n{% if interface[inventory_hostname]['mlag_side'] == 'A' %}\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n\n{% else %}\n\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n{% endif %}\n</code></pre>","location":"jinja-yaml/#network-config-template-example"},{"title":"Final Output - Tying It All Together","text":"<p>Lets put our full YAML data models below, as well as the output that is generated for <code>spine1-2</code> and <code>leaf1-2</code>:</p>  global.yml <pre><code># local users\nlocal_users:\n  admin:\n    privilege: 15\n    role: network-admin\n    secret: aristaadmin\n  noc:\n    privilege: 1\n    role: network-operator\n    secret: aristaops\n\n# aaa authentication and authorization\naaa_authentication:\n  login:\n    default: group radius local\n\naaa_authorization:\n  exec:\n    default: group radius local\n\n# radius servers\nradius_servers:\n  - host: 192.168.1.10\n    vrf: MGMT\n    key: radiusserverkey\n\n# HTTP Client source interface and VRF\nip_http_client_source_interfaces:\n    - name: Management1\n      vrf: MGMT\n\n# RADIUS source interface and VRF\nip_radius_source_interfaces:\n  - name: Management1\n    vrf: MGMT\n\n#MAC and ARP aging timers\nmac_address_table:\n  aging_time: 1800\n\narp:\n  aging:\n    timeout_default: 1500\n\n# DNS Servers\nname_servers:\n  - 10.100.100.20\n  - 8.8.8.8\n  - 4.4.4.4\n  - 208.67.222.222\n\n# DNS lookup source interface (Servers defined in 1L2P.yml)\nip_domain_lookup:\n  source_interfaces:\n    Management1:\n      vrf: MGMT\n\n# NTP Servers (source interface defined in group specific YML files (CORE, ACCESS, MGMT, INET)\nntp:\n  local_interface:\n    name: Management1\n    vrf: MGMT\n  servers:\n    - name: 0.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 1.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: 2.north-america.pool.ntp.org\n      vrf: MGMT\n    - name: time.google.com\n      vrf: MGMT\n\nclock:\n  timezone: \"America/Detroit\"\n</code></pre>   interface.yml <pre><code>---\nspine1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet48:\n      desc: To_SPINE2_MLAG_PEERLINK\n      mlag_peerlink: true\n    Ethernet1:\n      desc: TO_LEAF1\n      mlag_peerlink: false\n    Ethernet2:\n      desc: TO_LEAF2\n      mlag_peerlink: false\nspine2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_SPINE1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_LEAF1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_LEAF2'\n      mlag_peerlink: false\nleaf1:\n  mlag_side: A\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF2_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\nleaf2:\n  mlag_side: B\n  interfaces:\n    Ethernet47:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet48:\n      desc: 'To_LEAF1_MLAG_PEERLINK'\n      mlag_peerlink: true\n    Ethernet1:\n      desc: 'TO_SPINE1'\n      mlag_peerlink: false\n    Ethernet2:\n      desc: 'TO_SPINE2'\n      mlag_peerlink: false\n</code></pre>  <p>And the configuration outputs:</p>  spine1_config.cfg <pre><code>hostname spine1\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_LEAF1\n\n\n\ninterface Ethernet2\ndescription TO_LEAF2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre>   spine2_config.cfg <pre><code>hostname spine2\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_SPINE1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_LEAF1\n\n\n\ninterface Ethernet2\ndescription TO_LEAF2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre>   leaf1_config.cfg <pre><code>hostname leaf1\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_LEAF2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_LEAF2_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_SPINE1\n\n\n\ninterface Ethernet2\ndescription TO_SPINE2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.0/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.1\n</code></pre>   leaf2_config.cfg <pre><code>hostname leaf2\n\nusername admin priv 15 role network-admin secret aristaadmin\n\nusername noc priv 1 role network-operator secret aristaops\n\naaa authentication login default group radius local\n\naaa authorization exec default group radius local\n\nradius-server host 192.168.1.10 vrf MGMT key radiusserverkey\n\nip http client local-interface Management1 vrf MGMT\n\nip radius vrf MGMT source-interface Management1\n\nmac address-table aging-time 1800\n\narp aging timeout 1500\n\nip name-server 10.100.100.20\nip name-server 8.8.8.8\nip name-server 4.4.4.4\nip name-server 208.67.222.222\n\nip domain lookup vrf MGMT source-interface Management1\n\n\nntp server vrf MGMT 0.north-america.pool.ntp.org\nntp server vrf MGMT 1.north-america.pool.ntp.org\nntp server vrf MGMT 2.north-america.pool.ntp.org\nntp server vrf MGMT time.google.com\n\nclock timezone America/Detroit\n\nvlan 4094\nname MLAG\ntrunk group MLAGPeer\n\nno spanning-tree vlan 4094\n\ninterface Ethernet47\ndescription To_LEAF1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\ninterface Ethernet48\ndescription To_LEAF1_MLAG_PEERLINK\nchannel-group 2000 mode active\n\n\ninterface Ethernet1\ndescription TO_SPINE1\n\n\n\ninterface Ethernet2\ndescription TO_SPINE2\n\n\n\ninterface port-channel 2000\nswitchport\nswitchport mode trunk\nswitchport trunk group MLAGPeer\n\nint vlan 4094\nip address 192.0.0.1/31\n\nmlag configuration\ndomain-id workshop\nlocal-interface vlan4094\npeer-link po2000\npeer-address 192.0.0.0\n</code></pre>","location":"jinja-yaml/#final-output-tying-it-all-together"},{"title":"Welcome to VS Code","text":"<p></p>","location":"vscode/"},{"title":"What is VS Code?","text":"<p>Visual Studio Code (VS Code), written by Microsoft, is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages and runtime environments (such as C++, C#, Java, Python, PHP, Go, .NET).</p> <p>VS Code integrates directly with Git and allows Network Engineers to efficiently perform the following tasks:</p> <ul> <li>Edit Files</li> <li>Drag and Drop Files into the Explorer</li> <li>Visually Compare File Diffs</li> <li>Write &amp; Test Scripts</li> <li>Open Terminals for various shells</li> <li>Execute various Git commands (such as: status, init, log, commit, switch, add, etc...)</li> <li>Connect to Remote Hosts</li> </ul> <p>VS Code is your personal Git Genie making automation workflows effortless. Once you learn to use VS Code, you will wonder why it took so long to get on the bandwagon.</p>","location":"vscode/#what-is-vs-code"},{"title":"System Requirements","text":"<p>Visual Studio Code is a small download (&lt; 200 MB) and has a disk footprint of &lt; 500 MB. VS Code is lightweight and should run on today's hardware.</p> <p>Recommend Hardware:</p> <ul> <li>1.6 GHz or faster processor</li> <li>1 GB of RAM</li> </ul> <p>Support Platforms:</p> <ul> <li>macOS X (10.11+)</li> <li>Windows 8.0, 8.1 and 10, 11 (32-bit and 64-bit)</li> <li>Linux (Debian): Ubuntu Desktop 16.04, Debian 9</li> <li>Linux (Red Hat): Red Hat Enterprise Linux 7, CentOS 7, Fedora 34</li> </ul>","location":"vscode/#system-requirements"},{"title":"Download &amp; Installation","text":"<p>VS Code downloads for Windows, Linux and Mac can be found here.</p>","location":"vscode/#download-installation"},{"title":"VS Code GUI Walk-through","text":"<p>The four main areas of the VS Code GUI are:</p> <ul> <li>Activity Bar</li> <li>Status Bar</li> <li>Editor</li> <li>Terminal Window</li> </ul> <p></p>  Note <p>Your <code>Activity Bar</code> may have different icons for the extensions you have installed.</p>","location":"vscode/#vs-code-gui-walk-through"},{"title":"Extensions","text":"<p>There are tons of extensions to enhance your experience with VS Code. Below is a list of recommended extensions that are commonly used. For a full list of available extensions visit the VS Code Marketplace.</p> <ul> <li>Markdown All-in-One (Yu Zhang)</li> <li>YAML (Red Hat)</li> <li>Better Jinja (Samuel Colvin)</li> <li>Peacock (John Papa)</li> <li>Remote - SSH (Microsoft)</li> <li>Development Containers (Microsoft)</li> </ul>","location":"vscode/#extensions"},{"title":"Using Git source control in VS Code","text":"<p>Visual Studio Code has integrated source control management (SCM) and includes Git support out-of-the-box. In the previous section we ran all Git commands from the command-line. Now we will use VS Code to perform the same operations.</p>","location":"vscode/#using-git-source-control-in-vs-code"},{"title":"Start over with clean slate","text":"<p>Previously we downloaded and created a repository called <code>samplefiles</code>. Reset this repository to a directory by removing the hidden sub-directory <code>.git</code>.  This will remove any version control settings for the repository.</p> <pre><code>cd /home/coder/project/labfiles/samplefiles\nrm -rf .git\n</code></pre>","location":"vscode/#start-over-with-clean-slate"},{"title":"The Basics","text":"","location":"vscode/#the-basics"},{"title":"Initialize Repository","text":"<p>There are multiple ways to initialize a repository from VS Code. We will explore one of the methods.</p> <p>First open the folder <code>/home/coder/project/labfiles/samplefiles</code> from within the VS Code Explorer.</p> <p></p> <p>Re-open a new terminal window.</p> <p>Next click on the <code>Source Control</code> icon in the Activity Bar, and then click <code>Initialize Repository</code>.</p> <p></p> <p>This is equivalent to running <code>git init</code> from within that directory.</p> <p>Several things just happened. VS Code gives us a visual representation of the newly created Git repository. The <code>Source Control</code> icon now shows a blue dot with a <code>6</code>. This indicates six untracked, represented by the capital <code>U</code> next to each file.</p> <p>Also, in the bottom left corner of status bar we can see we are on the <code>main</code>* branch.</p> <p></p>","location":"vscode/#initialize-repository"},{"title":"VS Code Workflow","text":"<p>Use VS Code to perform the following actions.</p> <ul> <li>Stage files</li> <li>Commit staged files</li> <li>Check the log - git log</li> <li>Verify current branches</li> <li>Create and switch to new branch called <code>change-usernames</code></li> <li>Update username <code>arista</code> to <code>admin</code><ul> <li>Use VS Code Replace in files</li> <li>Show file diff</li> </ul> </li> <li>Stage and commit changes to new branch</li> <li>Switch back to <code>main</code> branch</li> <li>Merge <code>change-usernames</code> into <code>main</code><ul> <li>Command-Palette - git merge</li> </ul> </li> <li>Verify files have new names in the <code>main</code> branch</li> <li>Be a good citizen and clean up old branch</li> <li>Publish repo to GitHub</li> </ul>","location":"vscode/#vs-code-workflow"},{"title":"Dev Containers","text":"<p></p> <p>The Dev Container extension allows you to use containers as a development environment. It lets you use a Docker container as a full-featured development environment. Whether you deploy to containers or not, containers make a great development environment because you can:</p> <ul> <li>Develop with a consistent, easily reproducible toolchain on the same operating system you deploy to.</li> <li>Quickly swap between different, separate development environments and safely make updates without worrying about impacting your local machine.</li> <li>Make it easy for new team members / contributors to get up and running in a consistent development environment.</li> <li>Try out new technologies or clone a copy of a code base without impacting your local setup.</li> </ul> <p>The extension starts (or attaches to) a development container running a well defined tool and runtime stack. Workspace files can be mounted into the container from the local file system, or copied or cloned into it once the container is running. Extensions are installed and run inside the container where they have full access to the tools, platform, and file system.</p>","location":"vscode/#dev-containers"},{"title":"Dev Container System Requirements","text":"<p>Local:</p> <ul> <li>Windows: Docker Desktop 2.0+ on Windows 10 Pro/Enterprise. Windows 10 Home (2004+) requires Docker Desktop 2.2+ and the WSL2 back-end. (Docker Toolbox is not supported.)</li> <li>macOS: Docker Desktop 2.0+.</li> <li>Linux: Docker CE/EE 18.06+ and Docker Compose 1.21+. (The Ubuntu snap package is not supported.)</li> </ul> <p>Containers:</p> <ul> <li>x86_64 / ARMv7l (AArch32) / ARMv8l (AArch64) Debian 9+, Ubuntu 16.04+, CentOS / RHEL 7+</li> <li>x86_64 Alpine Linux 3.9+</li> </ul>  <p>Note</p> <p>Docker must be installed on your host for Dev Containers to work.</p>","location":"vscode/#dev-container-system-requirements"}]}